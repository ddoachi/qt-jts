---
# ============================================================================
# SPEC METADATA
# ============================================================================

# === IDENTIFICATION ===
id: E05-F01-T02
clickup_task_id: '86ew020fa'
title: Implement Lexer (Tokenization)
type: task

# === HIERARCHY ===
parent: E05-F01
children:
  - E05-F01-T02-S01
  - E05-F01-T02-S02
  - E05-F01-T02-S03
  - E05-F01-T02-S04
  - E05-F01-T02-S05
  - E05-F01-T02-S06
  - E05-F01-T02-S07
epic: E05
feature: F01
task: T02
domain: dsl

# === WORKFLOW ===
status: draft
priority: high

# === TRACKING ===
created: '2025-01-15'
updated: '2025-12-28'
due_date: ''
estimated_hours: 8
actual_hours: 0

# === METADATA ===
tags:
  - lexer
  - tokenizer
  - scanner
effort: large
risk: low
---

# Spec: E05-F01-T02 - Implement Lexer (Tokenization)

**Status**: Draft
**Type**: Task
**Parent**: E05-F01
**Created**: 2025-01-15
**Updated**: 2025-12-28

## Executive Summary

Implement the lexer (tokenizer) that converts DSL expression text into a stream of tokens. The lexer handles all token types including operators, keywords, identifiers, and literals. It provides position tracking for error reporting and performs efficient single-pass scanning of input text.

## Execution Flow

```
1. Initialize lexer with source text
   -> Set position, line, column to start
   -> If source is empty: Return [EOF] token
2. Scan next character
   -> If whitespace: Skip and continue
   -> If end of input: Return EOF token
3. Match token type
   -> If digit: Scan NUMBER (including decimals)
   -> If alpha/underscore: Scan IDENTIFIER or KEYWORD
   -> If ':': Scan PARAMETER name
   -> If '"' or "'": Scan STRING literal
   -> If operator char: Scan single or multi-char operator
   -> If delimiter: Return corresponding token
   -> If unknown: ERROR "Unexpected character at line X, column Y"
4. Create token with position info
   -> Store line, column, lexeme
   -> Return: SUCCESS with token list
```

## User Stories

### Primary User Story
**As a** DSL parser developer
**I want to** tokenize DSL expression strings
**So that** the parser receives a clean stream of typed tokens

### Additional Stories
- **As a** DSL user, **I want to** receive clear error messages with line/column, **So that** I can fix syntax errors quickly
- **As a** developer, **I want to** efficient single-pass tokenization, **So that** large expressions parse quickly

## Acceptance Scenarios

### Scenario 1: Simple Comparison
**Given** the input string "close > 100"
**When** I tokenize the expression
**Then** I receive [IDENTIFIER("close"), GT, NUMBER(100), EOF]

### Scenario 2: Function Call
**Given** the input string "sma(close, 20)"
**When** I tokenize the expression
**Then** I receive [IDENTIFIER("sma"), LPAREN, IDENTIFIER("close"), COMMA, NUMBER(20), RPAREN, EOF]

### Scenario 3: Parameter Syntax
**Given** the input string "rsi(:period)"
**When** I tokenize the expression
**Then** I receive [IDENTIFIER("rsi"), LPAREN, PARAMETER("period"), RPAREN, EOF]

### Scenario 4: Keyword Recognition
**Given** the input string "close > 100 AND volume > 1000"
**When** I tokenize the expression
**Then** AND is recognized as keyword token, not identifier

### Scenario 5: Error Reporting
**Given** the input string contains invalid character '@'
**When** I tokenize the expression
**Then** error includes line and column position of '@'

## Requirements

### Functional Requirements
- **FR-001**: System MUST tokenize NUMBER literals including decimals (e.g., 20.5)
- **FR-002**: System MUST tokenize STRING literals with single or double quotes
- **FR-003**: System MUST tokenize IDENTIFIER tokens (alphanumeric + underscore)
- **FR-004**: System MUST tokenize PARAMETER tokens with : prefix
- **FR-005**: System MUST recognize single-char operators (+, -, *, /, <, >, (, ), ,)
- **FR-006**: System MUST recognize multi-char operators (>=, <=, ==, !=)
- **FR-007**: System MUST recognize keywords (AND, OR, NOT, crosses_above, crosses_below)
- **FR-008**: System MUST skip whitespace between tokens
- **FR-009**: System MUST return EOF token at end of input

### Non-Functional Requirements
- **NFR-001**: Tokenization MUST be single-pass for O(n) performance
- **NFR-002**: Error messages MUST include line and column numbers
- **NFR-003**: Tokenization of 1000-character expression MUST complete <10ms

### Technical Constraints
- **TC-001**: Must use Python enum for TokenType
- **TC-002**: Must use dataclass for Token
- **TC-003**: Keywords must be case-sensitive as specified

## Key Entities

### Entity: TokenType
- **Description**: Enum of all possible token types
- **Key Attributes**: NUMBER, STRING, IDENTIFIER, PARAMETER, operators, keywords, delimiters, EOF
- **Relationships**: Used by Token dataclass

### Entity: Token
- **Description**: Single token with type, value, and position
- **Key Attributes**: type (TokenType), value (Any), line (int), column (int)
- **Relationships**: Produced by Lexer, consumed by Parser

### Entity: Lexer
- **Description**: Tokenizer class that scans source text
- **Key Attributes**: source (str), position (int), line (int), column (int)
- **Relationships**: Produces list of Tokens

## Dependencies

### Upstream Dependencies
- [ ] E05-F01-T01: AST node classes (for type system context)

### Downstream Impact
- [ ] E05-F01-T03: Parser consumes token stream from Lexer
- [ ] Error reporting affects user experience

## Gate Checks

### Pre-Implementation Gates
- [ ] No [NEEDS CLARIFICATION] markers remain
- [ ] All token types identified from DSL grammar
- [ ] Keyword list finalized
- [ ] PRD compliance verified

### Quality Gates
- [ ] All PRD example formulas tokenize correctly
- [ ] Error messages include position info
- [ ] Unit tests with 100% token type coverage
- [ ] Performance target met

## Subtasks Preview

| Subtask ID | Title | Effort | Status |
|------------|-------|--------|--------|
| E05-F01-T02-S01 | Create TokenType enum and Token dataclass | S | pending |
| E05-F01-T02-S02 | Implement single-character token scanning | S | pending |
| E05-F01-T02-S03 | Implement multi-character token scanning | M | pending |
| E05-F01-T02-S04 | Implement literal scanning (numbers, strings, identifiers) | M | pending |
| E05-F01-T02-S05 | Implement parameter scanning (:param_name) | S | pending |
| E05-F01-T02-S06 | Implement keyword recognition and whitespace handling | S | pending |
| E05-F01-T02-S07 | Add comprehensive error reporting with line/column tracking | M | pending |

## Success Criteria

### Acceptance Criteria
- [ ] Tokenize all example formulas from PRD Section 7.4
- [ ] Handle decimal numbers (e.g., 20.5)
- [ ] Handle parameters with : prefix
- [ ] Report errors with line and column numbers
- [ ] Unit tests with 100% token type coverage

### Definition of Done
- [ ] Code reviewed and approved
- [ ] Tests passing (unit tests)
- [ ] All token types documented
- [ ] Files created at libs/dsl/tokens.py and libs/dsl/lexer.py

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Missing token type | Low | Medium | Review all PRD examples |
| Ambiguous token matching | Low | Medium | Longest match rule |
| Performance issues | Low | Low | Single-pass design |

## Artifacts

### Input Documents
- [Parent Feature](../E05-F01.spec.md)
- [Dependency: T01](../T01/E05-F01-T01.spec.md)
- [Epic Spec](../../E05.spec.md) Section 4.1

### Output Artifacts
- [ ] `libs/dsl/tokens.py` - TokenType enum and Token dataclass
- [ ] `libs/dsl/lexer.py` - Lexer class implementation
- [ ] Unit tests for all token types

### Reference Implementation

```python
from enum import Enum, auto

class TokenType(Enum):
    # Literals
    NUMBER = auto()
    STRING = auto()
    IDENTIFIER = auto()
    PARAMETER = auto()

    # Operators
    PLUS = auto()       # +
    MINUS = auto()      # -
    STAR = auto()       # *
    SLASH = auto()      # /
    GT = auto()         # >
    LT = auto()         # <
    GTE = auto()        # >=
    LTE = auto()        # <=
    EQ = auto()         # ==
    NEQ = auto()        # !=

    # Keywords
    AND = auto()
    OR = auto()
    NOT = auto()
    CROSSES_ABOVE = auto()
    CROSSES_BELOW = auto()

    # Delimiters
    LPAREN = auto()     # (
    RPAREN = auto()     # )
    COMMA = auto()      # ,

    # Special
    EOF = auto()

KEYWORDS = {
    'AND': TokenType.AND,
    'OR': TokenType.OR,
    'NOT': TokenType.NOT,
    'crosses_above': TokenType.CROSSES_ABOVE,
    'crosses_below': TokenType.CROSSES_BELOW,
}
```

### Test Cases

```python
# Simple comparison
"close > 100" -> [IDENTIFIER, GT, NUMBER, EOF]

# Function call
"sma(close, 20)" -> [IDENTIFIER, LPAREN, IDENTIFIER, COMMA, NUMBER, RPAREN, EOF]

# Parameter
"rsi(:period)" -> [IDENTIFIER, LPAREN, PARAMETER, RPAREN, EOF]

# Keywords
"close > 100 AND volume > 1000" -> [IDENTIFIER, GT, NUMBER, AND, IDENTIFIER, GT, NUMBER, EOF]
```

---
*Template Version: 2.0.0 - Enhanced with Speckit features*
