# E06-F04: Signal Aggregator - Pre-Implementation Planning

## 1. Overview and Objectives

### 1.1 Purpose
Implement the `SignalAggregator` class to aggregate, group, and analyze signals from multiple symbol processing results. This component provides the foundation for presenting batch processing results in a meaningful way.

### 1.2 Key Objectives
- Aggregate `ProcessingResult` objects from multiple symbols into unified views
- Support flexible grouping by date, symbol, and signal type
- Compute statistics for signal analysis and reporting
- Provide sorting capabilities for signal display
- Handle edge cases gracefully (empty results, errors, missing data)

### 1.3 Success Metrics
- All acceptance criteria met (100%)
- Test coverage > 85%
- Handles 2,500+ symbol results efficiently (< 1 second aggregation time)
- Memory efficient aggregation (no data duplication)

---

## 2. Technical Approach

### 2.1 Core Design Principles

**Functional and Immutable**
- Return new data structures rather than modifying inputs
- Use dataclasses for type safety and immutability
- Enable composable operations (aggregate -> filter -> group -> sort)

**Performance Focused**
- Stream processing for large result sets
- Avoid unnecessary data copying
- Use efficient data structures (Counter for statistics, defaultdict for grouping)

**Error Resilient**
- Skip results with errors during aggregation
- Return empty structures for no matches (not None)
- Validate inputs and provide clear error messages

### 2.2 Architecture Pattern

The `SignalAggregator` follows a **pipeline pattern** where results flow through transformation stages:

```
ProcessingResult[] -> Filter (errors) -> Flatten (matches) ->
Sort -> AggregatedSignals -> Group/Statistics
```

### 2.3 Technology Choices

- **Python dataclasses**: For `AggregatedSignals` and `SignalStatistics`
- **collections.Counter**: For efficient counting in statistics
- **collections.defaultdict**: For grouping operations
- **Standard library only**: No external dependencies beyond domain model

---

## 3. Dependencies and Prerequisites

### 3.1 Required Dependencies

**E06-F01: Domain Model** (CRITICAL)
- `ProcessingResult`: Input data structure
- `SignalMatch`: Individual signal data
- `SignalType`: Enumeration for signal classification
- Candle data structure referenced in matches

### 3.2 Assumed Interfaces

```python
# From E06-F01
@dataclass
class ProcessingResult:
    symbol: str
    matches: list[SignalMatch]
    computed_indicators: dict[str, pd.Series]
    processing_time_ms: float
    error: Optional[str] = None

@dataclass
class SignalMatch:
    timestamp: datetime
    candle: Candle
    signal_type: SignalType
    indicator_values: dict[str, float]
    metadata: dict[str, Any] = field(default_factory=dict)
```

### 3.3 Prerequisites

1. E06-F01 must be implemented and tested
2. Domain model types must be importable from `src.processing.domain`
3. Test fixtures for `ProcessingResult` and `SignalMatch` available

---

## 4. Implementation Plan

### 4.1 Phase 1: Data Structures (E06-F04-T01)

**Task**: Define `AggregatedSignals` and `SignalStatistics`

**Steps**:
1. Create `src/processing/aggregator.py`
2. Define `AggregatedSignals` dataclass:
   - `total_symbols: int`
   - `symbols_with_matches: int`
   - `total_matches: int`
   - `matches: list[dict]`
3. Define `SignalStatistics` dataclass:
   - `total: int`
   - `by_symbol: dict[str, int]`
   - `by_date: dict[date, int]`
4. Add type hints and docstrings
5. Write unit tests for data structure creation

**Estimated Effort**: 1-2 hours

### 4.2 Phase 2: Core Aggregation (E06-F04-T01)

**Task**: Implement `SignalAggregator.aggregate()`

**Steps**:
1. Implement `__init__()` (stateless, may be empty or config-based)
2. Implement `aggregate()` method:
   - Accept `results: list[ProcessingResult]`
   - Accept `sort_by: str = 'timestamp'` parameter
   - Filter out results with errors
   - Flatten matches from all results
   - Create match dictionaries with symbol, timestamp, candle, etc.
   - Sort by requested field
   - Return `AggregatedSignals`
3. Handle edge cases:
   - Empty results list
   - All results have errors
   - Invalid `sort_by` field
4. Write unit tests for core aggregation

**Estimated Effort**: 3-4 hours

### 4.3 Phase 3: Grouping Functions (E06-F04-T02)

**Task**: Implement `group_by_date()` and related grouping

**Steps**:
1. Implement `group_by_date()`:
   - Accept `signals: AggregatedSignals`
   - Group matches by `timestamp.date()`
   - Return `dict[date, list[dict]]`
   - Sort dates in result
2. Consider `group_by_symbol()` for completeness:
   - Group matches by symbol
   - Return `dict[str, list[dict]]`
3. Write unit tests for grouping functions

**Estimated Effort**: 2-3 hours

### 4.4 Phase 4: Statistics Computation (E06-F04-T02)

**Task**: Implement `get_statistics()`

**Steps**:
1. Implement `get_statistics()`:
   - Accept `signals: AggregatedSignals`
   - Use `Counter` for efficient counting
   - Count by symbol
   - Count by date
   - Return `SignalStatistics`
2. Handle empty signals gracefully
3. Write unit tests for statistics

**Estimated Effort**: 2-3 hours

### 4.5 Phase 5: Integration Testing

**Task**: End-to-end tests with realistic data

**Steps**:
1. Create integration tests with multiple symbols
2. Test full pipeline: aggregate -> group -> statistics
3. Performance test with 100+ symbols
4. Verify memory efficiency

**Estimated Effort**: 2-3 hours

---

## 5. File Structure and Locations

### 5.1 Source Files

```
src/
└── processing/
    ├── __init__.py
    ├── domain.py              # E06-F01 (existing/dependency)
    └── aggregator.py          # NEW: This feature
        ├── AggregatedSignals
        ├── SignalStatistics
        └── SignalAggregator
```

### 5.2 Test Files

```
tests/
└── processing/
    ├── __init__.py
    ├── test_aggregator.py     # NEW: Unit tests
    └── test_aggregator_integration.py  # NEW: Integration tests
```

### 5.3 Configuration

No configuration files needed for this feature.

---

## 6. Key Interfaces and Contracts

### 6.1 Public API

```python
class SignalAggregator:
    """Aggregate and analyze signals from multiple symbol processing results."""

    def aggregate(
        self,
        results: list[ProcessingResult],
        sort_by: str = 'timestamp'
    ) -> AggregatedSignals:
        """
        Combine results from multiple symbols into a unified view.

        Args:
            results: List of processing results from multiple symbols
            sort_by: Field to sort by ('timestamp', 'symbol', 'signal_type')

        Returns:
            AggregatedSignals with combined matches and metadata

        Raises:
            ValueError: If sort_by field is invalid
        """
        pass

    def group_by_date(
        self,
        signals: AggregatedSignals
    ) -> dict[date, list[dict]]:
        """
        Group signals by date for timeline view.

        Args:
            signals: Aggregated signals to group

        Returns:
            Dictionary mapping dates to lists of matches
        """
        pass

    def get_statistics(
        self,
        signals: AggregatedSignals
    ) -> SignalStatistics:
        """
        Compute statistics on aggregated signals.

        Args:
            signals: Aggregated signals to analyze

        Returns:
            SignalStatistics with counts by symbol and date
        """
        pass
```

### 6.2 Data Contracts

**AggregatedSignals**
```python
@dataclass
class AggregatedSignals:
    """Aggregated view of signals from multiple symbols."""
    total_symbols: int              # Always >= 0
    symbols_with_matches: int       # 0 <= value <= total_symbols
    total_matches: int              # Always >= 0
    matches: list[dict]             # Length equals total_matches
```

**SignalStatistics**
```python
@dataclass
class SignalStatistics:
    """Statistical summary of signals."""
    total: int                      # Always >= 0
    by_symbol: dict[str, int]       # Empty dict if no matches
    by_date: dict[date, int]        # Empty dict if no matches
```

**Match Dictionary Format**
```python
{
    'symbol': str,                  # Symbol code
    'timestamp': datetime,          # Signal timestamp
    'candle': Candle,               # OHLCV data at signal
    'signal_type': SignalType,      # ENTRY, EXIT, SCAN_HIT
    'indicators': dict[str, float]  # Indicator values at signal
}
```

### 6.3 Error Handling

- **Invalid sort_by field**: Raise `ValueError` with message listing valid options
- **Empty results**: Return `AggregatedSignals` with all zero counts and empty list
- **All errors**: Return `AggregatedSignals` with total_symbols set but zero matches
- **None inputs**: Raise `TypeError` with clear message

---

## 7. Testing Strategy

### 7.1 Unit Test Coverage

**Test Cases for `aggregate()`**:
- Aggregate results from multiple symbols (2, 5, 10 symbols)
- Sort by timestamp (default)
- Sort by symbol
- Sort by signal_type
- Handle empty results list
- Skip results with errors
- Handle all results having errors
- Validate match dictionary structure
- Invalid sort_by field raises ValueError

**Test Cases for `group_by_date()`**:
- Group signals across multiple dates
- Handle signals on same date
- Return sorted dates
- Handle empty aggregated signals
- Preserve all match data in groups

**Test Cases for `get_statistics()`**:
- Calculate total count
- Calculate per-symbol counts
- Calculate per-date counts
- Handle empty signals
- Multiple symbols with varying match counts
- Verify Counter usage for efficiency

### 7.2 Integration Tests

**End-to-End Scenarios**:
1. Batch scan with 50 symbols, aggregate, group, and compute stats
2. Performance test with 500 symbols
3. Mixed error and success results
4. All symbols with no matches
5. Single symbol edge case

### 7.3 Test Data Fixtures

```python
@pytest.fixture
def sample_processing_results():
    """Generate realistic processing results for testing."""
    return [
        ProcessingResult(
            symbol="005930",
            matches=[
                SignalMatch(
                    timestamp=datetime(2024, 1, 15, 9, 0),
                    candle=create_test_candle(105000, 106000, 104500, 105500),
                    signal_type=SignalType.SCAN_HIT,
                    indicator_values={'rsi': 65.3, 'sma_20': 104800}
                ),
                # More matches...
            ],
            computed_indicators={},
            processing_time_ms=45.2
        ),
        # More results...
    ]
```

### 7.4 Test Coverage Goals

- Line coverage: > 90%
- Branch coverage: > 85%
- All public methods tested
- All edge cases covered
- Integration test for realistic workflow

---

## 8. Risks and Mitigations

### 8.1 Risk: Memory Usage with Large Result Sets

**Impact**: High - Could cause OOM with 2,500+ symbols

**Likelihood**: Medium

**Mitigation**:
- Avoid copying data unnecessarily
- Use references to existing match objects where possible
- Consider generator-based streaming for very large sets
- Add memory profiling tests
- Document memory characteristics in API docs

### 8.2 Risk: Performance Degradation with Sorting

**Impact**: Medium - Slow aggregation defeats purpose

**Likelihood**: Low to Medium

**Mitigation**:
- Use efficient sorting (Python's Timsort is optimized)
- Consider lazy sorting (sort on demand)
- Benchmark with realistic data sizes
- Profile and optimize hot paths
- Document performance characteristics (O(n log n) for sorting)

### 8.3 Risk: Incomplete Domain Model Dependency

**Impact**: High - Cannot implement without proper types

**Likelihood**: Low (assuming E06-F01 is complete)

**Mitigation**:
- Verify E06-F01 completion before starting
- Review domain model interfaces thoroughly
- Create mock implementations for early testing
- Maintain communication with F01 implementer

### 8.4 Risk: Match Dictionary Structure Changes

**Impact**: Medium - Breaks downstream consumers

**Likelihood**: Low to Medium

**Mitigation**:
- Document match dictionary structure clearly
- Consider creating a typed dataclass instead of dict
- Version the API if structure needs to change
- Add validation tests for dictionary structure
- Use TypedDict for type hints in Python 3.8+

### 8.5 Risk: Date Grouping Timezone Issues

**Impact**: Medium - Incorrect grouping across timezones

**Likelihood**: Medium

**Mitigation**:
- Document timezone expectations (UTC assumed)
- Ensure `ProcessingResult.timestamp` is timezone-aware
- Add tests for timezone edge cases
- Consider timezone parameter in `group_by_date()`

### 8.6 Risk: Test Coverage Below Target

**Impact**: Medium - Bugs in production

**Likelihood**: Low

**Mitigation**:
- Write tests first (TDD approach)
- Use coverage tools during development
- Require coverage checks in CI/CD
- Review coverage reports before completion

---

## 9. Additional Considerations

### 9.1 Future Enhancements

- `group_by_symbol()`: Group by symbol instead of date
- `group_by_signal_type()`: Group by signal type
- `filter_by_indicator()`: Filter matches by indicator criteria
- `sort_by_multiple()`: Multi-field sorting
- Streaming/lazy evaluation for very large datasets
- Export to DataFrame for analysis

### 9.2 Documentation Requirements

- API documentation with docstrings (Google style)
- Usage examples in docstrings
- Integration guide in feature README
- Performance characteristics documented
- Memory usage guidelines

### 9.3 Code Quality Standards

- Type hints on all public methods
- Docstrings on all public methods and classes
- Follow PEP 8 style guide
- Use descriptive variable names
- Keep methods under 50 lines where possible
- Single responsibility principle

---

## 10. Implementation Checklist

### Pre-Implementation
- [ ] Review E06-F01 domain model implementation
- [ ] Verify dependencies are available
- [ ] Set up test fixtures
- [ ] Create feature branch

### Phase 1: Data Structures
- [ ] Create `src/processing/aggregator.py`
- [ ] Define `AggregatedSignals` dataclass
- [ ] Define `SignalStatistics` dataclass
- [ ] Write unit tests for data structures
- [ ] Verify tests pass

### Phase 2: Core Aggregation
- [ ] Implement `SignalAggregator.__init__()`
- [ ] Implement `SignalAggregator.aggregate()`
- [ ] Handle error filtering
- [ ] Implement sorting logic
- [ ] Write unit tests for aggregation
- [ ] Verify tests pass

### Phase 3: Grouping
- [ ] Implement `group_by_date()`
- [ ] Write unit tests for grouping
- [ ] Verify tests pass

### Phase 4: Statistics
- [ ] Implement `get_statistics()`
- [ ] Write unit tests for statistics
- [ ] Verify tests pass

### Phase 5: Integration
- [ ] Write integration tests
- [ ] Run performance benchmarks
- [ ] Verify memory usage
- [ ] Check test coverage (target: > 85%)

### Final Review
- [ ] Code review with team
- [ ] Update documentation
- [ ] Verify all acceptance criteria met
- [ ] Merge to main branch

---

## 11. References

- **E06.spec.md Section 4.3**: Signal Aggregator reference implementation
- **E06-F01.spec.md**: Domain Model specification
- **E06-F04.spec.md**: This feature specification
- **Python dataclasses**: https://docs.python.org/3/library/dataclasses.html
- **collections.Counter**: https://docs.python.org/3/library/collections.html#collections.Counter

---

**Document Version**: 1.0
**Created**: 2025-12-28
**Author**: Pre-implementation planning for E06-F04
**Status**: Ready for implementation
