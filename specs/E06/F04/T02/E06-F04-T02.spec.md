---
id: E06-F04-T02
clickup_task_id: '86ew020n1'
title: Implement Grouping and Statistics
type: task
parent: E06-F04
children: []
epic: E06
feature: F04
task: T02
domain: Signal Aggregation
status: Draft
priority: Medium
dates:
  start: ""
  due: ""
  completed: ""
hours:
  estimated: 8
  actual: 0
tags:
  - aggregation
  - statistics
  - grouping
effort: M
risk: Low
---

# Spec: E06-F04-T02 - Implement Grouping and Statistics

**Status:** Draft | **Effort:** Medium | **Risk:** Low

---

## Executive Summary

This task adds grouping and statistical analysis capabilities to the SignalAggregator, enabling timeline views, symbol-based analysis, and signal distribution insights. The implementation includes methods to group signals by date, symbol, and signal type, along with computed statistics for analysis.

---

## Execution Flow

1. Define `SignalStatistics` dataclass with fields for total, by_symbol, by_date, and by_signal_type
2. Implement grouping methods: `group_by_date()`, `group_by_symbol()`, `group_by_signal_type()`
3. Implement `get_statistics()` method to compute statistics on signals
4. Add computed properties: `top_symbols`, `date_range`
5. Write comprehensive unit tests for all methods
6. Verify graceful handling of empty signal sets

---

## User Stories

As a user, I want to:
- View signals organized by date for timeline analysis
- Analyze signals grouped by symbol to understand per-symbol behavior
- Group signals by type to identify signal distribution patterns
- See top-performing symbols ranked by match count
- Determine the date range covered by signals

---

## Acceptance Scenarios

1. **Group By Date** - Given aggregated signals with multiple dates, when grouped by date, signals are correctly partitioned and sorted chronologically
2. **Group By Symbol** - Given aggregated signals with multiple symbols, when grouped by symbol, each symbol has its associated signals
3. **Group By Signal Type** - Given aggregated signals with multiple types, when grouped by type, signals are correctly categorized
4. **Statistics Computation** - Given aggregated signals, when statistics are computed, all counts (total, by_symbol, by_date, by_signal_type) are accurate
5. **Top Symbols Property** - Given statistics, the top_symbols property returns symbols sorted by match count in descending order
6. **Date Range Property** - Given statistics with multiple dates, date_range returns the earliest and latest dates
7. **Empty Signal Handling** - Given an empty signal set, all methods return appropriate empty structures without errors

---

## Requirements

### Functional Requirements

1. **SignalStatistics Dataclass**
   - Immutable (frozen=True)
   - Fields: total (int), by_symbol (dict), by_date (dict), by_signal_type (dict)
   - Computed property: top_symbols returning sorted list of (symbol, count) tuples
   - Computed property: date_range returning Optional[tuple[date, date]]

2. **Grouping Methods**
   - `group_by_date(signals)`: Returns dict[date, list[dict]] sorted by date ascending
   - `group_by_symbol(signals)`: Returns dict[str, list[dict]]
   - `group_by_signal_type(signals)`: Returns dict[SignalType, list[dict]]

3. **Statistics Computation**
   - `get_statistics(signals)`: Returns SignalStatistics with all counts computed
   - Must handle empty signals gracefully

### Non-Functional Requirements

- All methods must have O(n) time complexity
- Statistics dataclass must be hashable and immutable
- Methods must handle edge cases (empty signals, single signal, etc.)

---

## Key Entities

### SignalStatistics (Dataclass)

```python
@dataclass(frozen=True)
class SignalStatistics:
    """Statistics computed from signals"""
    total: int
    by_symbol: dict[str, int]
    by_date: dict[date, int]
    by_signal_type: dict[SignalType, int]

    @property
    def top_symbols(self) -> list[tuple[str, int]]:
        """Symbols with most matches, sorted descending"""
        return sorted(
            self.by_symbol.items(),
            key=lambda x: x[1],
            reverse=True
        )

    @property
    def date_range(self) -> Optional[tuple[date, date]]:
        """Date range of signals"""
        if not self.by_date:
            return None
        dates = list(self.by_date.keys())
        return (min(dates), max(dates))
```

### SignalAggregator Methods

- `group_by_date(signals: AggregatedSignals) -> dict[date, list[dict]]`
- `group_by_symbol(signals: AggregatedSignals) -> dict[str, list[dict]]`
- `group_by_signal_type(signals: AggregatedSignals) -> dict[SignalType, list[dict]]`
- `get_statistics(signals: AggregatedSignals) -> SignalStatistics`

---

## Dependencies

- **E06-F04-T01**: SignalAggregator base implementation (must be completed first)
- **Related**: E06 epic context and overall signal processing architecture

---

## Gate Checks

- [ ] Code review completed
- [ ] All acceptance criteria met
- [ ] Unit tests passing (100% coverage of new code)
- [ ] Integration tests with E06-F04-T01 passing
- [ ] Documentation updated

---

## Tasks Preview

1. Create SignalStatistics dataclass with properties
2. Implement group_by_date() method
3. Implement group_by_symbol() method
4. Implement group_by_signal_type() method
5. Implement get_statistics() method
6. Write unit tests for grouping methods
7. Write unit tests for statistics computation
8. Handle edge cases and empty signals

---

## Success Criteria

- All grouping methods return correctly partitioned signals
- Statistics are computed accurately for all dimensions
- Properties (top_symbols, date_range) compute correctly
- Empty signals are handled gracefully with no errors
- All acceptance criteria tests pass
- Code is well-documented with docstrings
- Performance meets O(n) time complexity
- 100% unit test coverage of new code

---

## Risk Assessment

### Risks

1. **Data Consistency** - Risk that grouped data could become inconsistent if source signals change
   - Mitigation: Ensure immutability of statistics and snapshot-based grouping

2. **Performance** - Risk of poor performance with very large signal sets
   - Mitigation: Keep algorithms O(n), use efficient data structures (Counter, defaultdict)

3. **Edge Cases** - Risk of missing edge cases like empty signals or single entries
   - Mitigation: Comprehensive test coverage including edge cases

### Overall Risk: Low

The implementation is straightforward with well-defined requirements and clear dependencies.

---

## Notes and Clarifications

### Implementation Notes

1. Use `Counter` from collections for efficient counting
2. Use `defaultdict(list)` for grouping
3. Sort by date in ascending order for timeline view (earliest first)
4. Top symbols should be sorted descending (most matches first)
5. Date range should use min/max of keys in by_date dict

### Questions Resolved

- Grouping methods should return dict, not custom GroupedSignals type (simpler, more flexible)
- Statistics should be frozen (immutable) to prevent accidental modification
- Empty signals should return initialized SignalStatistics(total=0) not None

---

## Artifacts

### Code Files to Create/Modify

- `signal_aggregator.py` - Add SignalStatistics dataclass and grouping methods

### Test Files

- `test_signal_aggregator_grouping.py` - Tests for grouping methods
- `test_signal_aggregator_statistics.py` - Tests for statistics computation

### Documentation

- Update SignalAggregator docstring to document new methods

---

## Metadata

| Field | Value |
|-------|-------|
| Task ID | E06-F04-T02 |
| Title | Implement Grouping and Statistics |
| Status | Draft |
| Feature | E06-F04: Signal Aggregator |
| Epic | E06 |
| Effort | M (Medium) |
| Priority | Medium |
| Risk | Low |
| Dependencies | E06-F04-T01 |
| Estimated Hours | 8 |
| Created | 2025-12-28 |
| Last Updated | 2025-12-28 |
