# Spec: E06-F02-T02-S01 - ProcessPoolExecutor Setup

---
id: E06-F02-T02-S01
clickup_task_id: null
title: ProcessPoolExecutor Setup
type: subtask
parent: E06-F02-T02
children: []
epic: E06
feature: F02
task: T02
subtask: S01
domain: parallel-processing
status: Draft
priority: medium
dates:
  start: null
  end: null
  milestone: null
hours: null
tags:
  - ProcessPoolExecutor
  - parallelization
  - executor-lifecycle
  - serialization
effort: S
risk: low
---

**Status:** Draft

---

## Executive Summary

Set up the ProcessPoolExecutor infrastructure for parallel symbol processing. This includes executor lifecycle management and serialization considerations. The goal is to initialize ProcessPoolExecutor with configurable workers, implement context manager for lifecycle, handle executor shutdown properly, and address serialization requirements.

---

## Execution Flow

1. **Initialize ProcessPoolExecutor**: Create executor with configurable max_workers (defaults to CPU count)
2. **Implement Context Manager**: Provide __enter__ and __exit__ methods for proper resource management
3. **Support Lazy Creation**: Create executor lazily if not using context manager
4. **Handle Shutdown**: Ensure proper executor shutdown with wait for pending tasks
5. **Prepare Serialization**: Create helper functions to serialize data for worker processes
6. **Define Worker Function**: Implement module-level worker function that can be pickled

---

## User Stories

As a system architect, I need to establish a robust executor infrastructure so that symbol processing can be parallelized with proper resource management and lifecycle control.

As a developer, I need the ProcessPoolExecutor to be easy to use with context managers so that resources are properly cleaned up even in error scenarios.

As an operator, I need configurable worker counts so that the system can be tuned for different hardware environments.

---

## Acceptance Scenarios

**Scenario 1: Context Manager Initialization**
- Given a ProcessingEngine instance
- When entering the context manager
- Then ProcessPoolExecutor should be created with specified max_workers
- And debug log should indicate executor started

**Scenario 2: Context Manager Shutdown**
- Given a ProcessingEngine inside a with block
- When exiting the context manager
- Then ProcessPoolExecutor should shutdown with wait=True
- And executor reference should be cleared
- And debug log should indicate shutdown complete

**Scenario 3: Lazy Executor Creation**
- Given a ProcessingEngine created without context manager
- When calling _ensure_executor()
- Then ProcessPoolExecutor should be created
- And warning log should suggest using context manager

**Scenario 4: Manual Shutdown**
- Given a ProcessingEngine with initialized executor
- When calling shutdown(wait=True)
- Then executor should shutdown properly
- And executor reference should be cleared

**Scenario 5: Serialization of Worker Arguments**
- Given a ProcessingJob with complex objects
- When calling _prepare_symbol_args()
- Then all arguments should be converted to picklable primitives
- And dates should be ISO format strings
- And nested objects should be flattened to primitive types

---

## Requirements

### Functional Requirements

1. ProcessPoolExecutor must be initialized with configurable max_workers parameter
2. If max_workers is None, default to CPU count (or 4 as fallback)
3. Context manager interface (__enter__, __exit__) must be implemented
4. Executor must be created lazily if not using context manager
5. Shutdown must wait for all pending tasks to complete
6. Serialization helpers must convert complex objects to picklable primitives
7. Module-level worker function must be importable and picklable
8. Logging must track executor lifecycle events

### Non-Functional Requirements

1. Executor initialization should be efficient (minimal overhead)
2. Shutdown should complete gracefully without hanging
3. Resource leaks must not occur even on exception
4. Worker process recreation logic must be deterministic

---

## Key Entities

### ProcessingEngine

The main class that manages the ProcessPoolExecutor lifecycle:

```python
class ProcessingEngine(IProcessingEngine):
    """Processing engine with parallel execution support"""

    def __init__(
        self,
        candle_repo: ICandleRepository,
        formula_service: FormulaService,
        max_workers: int = None
    )

    def __enter__(self) -> 'ProcessingEngine'
    def __exit__(exc_type, exc_val, exc_tb) -> bool
    def _ensure_executor(self) -> ProcessPoolExecutor
    def shutdown(self, wait: bool = True) -> None
```

### ProcessingJob

Contains job parameters including timeframe, date range, formula, and parameters.

### ProcessingResult

Serializable result returned by worker processes.

---

## Dependencies

- **E06-F02-T01**: ProcessingEngine Core - Core engine implementation that this subtask extends
- **E06-F02-T02**: Implement Parallel Processing - Parent task
- **E06-F02-T02-S02**: Parallel Batch Execution - Sibling subtask for batch processing
- Python stdlib: concurrent.futures.ProcessPoolExecutor
- ICandleRepository interface
- FormulaService

---

## Gate Checks

- [ ] Code review: ProcessPoolExecutor lifecycle implementation
- [ ] Code review: Serialization helpers
- [ ] Unit tests: All test cases passing
- [ ] Performance: Executor initialization overhead < 100ms
- [ ] Memory: No resource leaks on repeated initialization/shutdown cycles

---

## Tasks Preview

1. Implement ProcessingEngine class with __init__, __enter__, __exit__
2. Implement _ensure_executor() method for lazy creation
3. Implement shutdown() method for manual shutdown
4. Create _prepare_symbol_args() serialization helper
5. Define _worker_process_symbol() module-level worker function
6. Implement unit tests for lifecycle management
7. Add integration tests with actual ProcessPoolExecutor

---

## Success Criteria

- ProcessPoolExecutor successfully created and destroyed in context manager
- Executor properly waits for pending tasks on shutdown
- All arguments are properly serialized to picklable types
- Unit tests achieve 100% code coverage for lifecycle code
- No resource leaks detected in stress tests (1000+ cycles)
- Logging provides clear visibility into executor lifecycle
- Documentation includes usage examples with context manager

---

## Risk Assessment

### Risk 1: Deadlock on Shutdown

**Severity:** High | **Probability:** Medium | **Impact:** Application hang

**Mitigation:**
- Use shutdown(wait=True) consistently
- Add timeout wrapper if needed
- Monitor logs for shutdown delays

### Risk 2: Serialization Issues

**Severity:** High | **Probability:** Medium | **Impact:** Runtime errors in worker processes

**Mitigation:**
- Use _prepare_symbol_args() for all worker arguments
- Test all data types that will be passed to workers
- Document pickling limitations
- Consider using cloudpickle if needed

### Risk 3: Worker Process Initialization Failure

**Severity:** High | **Probability:** Low | **Impact:** Silent failures, lost results

**Mitigation:**
- Validate configuration before creating worker processes
- Log configuration details in worker process startup
- Implement error handling in worker function
- Return structured errors from worker processes

### Risk 4: Resource Leaks from Missing Shutdown

**Severity:** Medium | **Probability:** Low | **Impact:** Resource exhaustion over time

**Mitigation:**
- Always use context manager pattern
- Add warnings when executor created outside context
- Consider using weakref or finalizers for cleanup

---

## Notes and Clarifications

### Serialization Considerations

ProcessPoolExecutor uses pickle to serialize arguments and results across process boundaries. This means:

- All arguments must be picklable
- Complex objects (repository, service) cannot be passed directly
- Instead, pass configuration dicts and recreate in worker process
- Consider using cloudpickle if standard pickle fails

### Worker Process Lifecycle

Each worker process:
1. Receives pickled arguments
2. Recreates dependencies from configuration
3. Processes the task
4. Returns picklable result

This adds overhead but ensures clean process isolation.

### CPU Count Default

Using `os.cpu_count()` provides good default parallelism. Fallback to 4 for safety if cpu_count returns None on unusual systems.

### Context Manager Best Practice

Always use context manager to ensure resources are cleaned up:

```python
with ProcessingEngine(repo, service) as engine:
    results = await engine.process_batch(job)
```

This ensures shutdown is called even on exceptions.

---

## Implementation Details

### Executor Lifecycle

```python
from concurrent.futures import ProcessPoolExecutor

class ProcessingEngine(IProcessingEngine):
    """Processing engine with parallel execution support"""

    def __init__(
        self,
        candle_repo: ICandleRepository,
        formula_service: FormulaService,
        max_workers: int = None
    ):
        self._candle_repo = candle_repo
        self._formula_service = formula_service
        self._max_workers = max_workers or (os.cpu_count() or 4)
        self._executor: Optional[ProcessPoolExecutor] = None

        logger.info(
            f"ProcessingEngine initialized: max_workers={self._max_workers}"
        )

    def __enter__(self) -> 'ProcessingEngine':
        """
        Enter context manager - create executor.

        Usage:
            with ProcessingEngine(repo, service) as engine:
                results = await engine.process_batch(job)
        """
        self._executor = ProcessPoolExecutor(max_workers=self._max_workers)
        logger.debug(f"ProcessPoolExecutor started with {self._max_workers} workers")
        return self

    def __exit__(
        self,
        exc_type: Optional[type],
        exc_val: Optional[BaseException],
        exc_tb: Optional[Any]
    ) -> bool:
        """
        Exit context manager - shutdown executor.

        Waits for all pending tasks to complete before shutdown.
        """
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None
            logger.debug("ProcessPoolExecutor shutdown complete")
        return False  # Don't suppress exceptions

    def _ensure_executor(self) -> ProcessPoolExecutor:
        """
        Ensure executor is available.

        Creates executor lazily if not using context manager.
        """
        if self._executor is None:
            self._executor = ProcessPoolExecutor(max_workers=self._max_workers)
            logger.warning(
                "ProcessPoolExecutor created outside context manager. "
                "Consider using 'with ProcessingEngine(...) as engine:'"
            )
        return self._executor

    def shutdown(self, wait: bool = True) -> None:
        """
        Manually shutdown executor.

        Use this if not using context manager.
        """
        if self._executor:
            self._executor.shutdown(wait=wait)
            self._executor = None
```

### Serialization Helper

For ProcessPoolExecutor, functions and data must be picklable:

```python
def _prepare_symbol_args(
    self,
    symbol: str,
    job: ProcessingJob
) -> dict[str, Any]:
    """
    Prepare arguments for worker process.

    Converts complex objects to picklable primitives.
    """
    return {
        'symbol': symbol,
        'timeframe': job.timeframe,
        'date_range_start': job.date_range.start.isoformat(),
        'date_range_end': job.date_range.end.isoformat(),
        'formula_expression': job.formula.expression if job.formula else None,
        'formula_id': job.formula.id if job.formula else None,
        'parameters': job.parameters
    }


# Module-level function for multiprocessing (must be importable)
def _worker_process_symbol(
    args: dict[str, Any],
    candle_repo_config: dict,
    formula_service_config: dict
) -> ProcessingResult:
    """
    Worker function for processing a single symbol.

    This runs in a separate process, so it needs to:
    1. Recreate repository and service from config
    2. Process the symbol
    3. Return serializable result
    """
    # Recreate dependencies in worker process
    candle_repo = create_candle_repository(candle_repo_config)
    formula_service = create_formula_service(formula_service_config)

    # Process symbol
    symbol = args['symbol']
    date_range = DateRange(
        start=date.fromisoformat(args['date_range_start']),
        end=date.fromisoformat(args['date_range_end'])
    )

    # ... processing logic ...
```

---

## Artifacts

### Test Coverage

```python
def test_executor_context_manager():
    """Test context manager creates and shuts down executor"""
    engine = ProcessingEngine(MagicMock(), MagicMock(), max_workers=2)

    assert engine._executor is None

    with engine:
        assert engine._executor is not None
        assert isinstance(engine._executor, ProcessPoolExecutor)

    assert engine._executor is None

def test_executor_lazy_creation():
    """Test executor is created lazily outside context"""
    engine = ProcessingEngine(MagicMock(), MagicMock(), max_workers=2)

    executor = engine._ensure_executor()

    assert executor is not None
    engine.shutdown()

def test_executor_manual_shutdown():
    """Test manual shutdown"""
    engine = ProcessingEngine(MagicMock(), MagicMock(), max_workers=2)
    engine._ensure_executor()

    engine.shutdown(wait=True)

    assert engine._executor is None
```

### Code Files to Create/Modify

1. `processing_engine.py` - Main ProcessingEngine class implementation
2. `worker_functions.py` - Module-level worker functions
3. `test_processing_engine.py` - Unit tests for executor lifecycle

---

## Metadata

| Field | Value |
|-------|-------|
| Spec ID | E06-F02-T02-S01 |
| Title | ProcessPoolExecutor Setup |
| Type | Subtask |
| Parent Task | E06-F02-T02 |
| Epic | E06 |
| Feature | F02 |
| Task | T02 |
| Subtask | S01 |
| Status | Draft |
| Priority | Medium |
| Effort | S (Small) |
| Risk | Low |
| Domain | parallel-processing |
| Created | 2025-12-28 |
| Updated | 2025-12-28 |
