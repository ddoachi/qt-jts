# Spec: E06-F02-T02-S02 - Parallel Batch Execution

---

## Metadata

```yaml
id: E06-F02-T02-S02
clickup_task_id: null
title: Parallel Batch Execution
type: subtask
parent: E06-F02-T02
children: []
epic: E06
feature: F02
task: T02
subtask: S02
domain: Processing
status: Draft
priority: Medium
dates:
  created: null
  started: null
  completed: null
hours:
  estimated: 8
  logged: 0
tags:
  - parallel-processing
  - batch-execution
  - concurrency
effort: M
risk: Medium
```

---

## Status

**Current Status:** Draft

---

## Executive Summary

Implement parallel batch execution using the ProcessPoolExecutor. This enables concurrent processing of multiple symbols across CPU cores for significant performance improvement. The implementation will submit jobs to executor in parallel, collect results as they complete, handle timeouts and errors, and track progress for all completions.

---

## Execution Flow

1. Submit all symbols to the process pool executor
2. Use `as_completed()` to collect results as they finish
3. Implement timeout handling (60 seconds per symbol)
4. Collect errors without stopping the batch
5. Report progress via callback for each completion
6. Log summary statistics

---

## User Stories

- As a user, I want symbols to be processed in parallel across available CPU cores so that large batches complete faster
- As a developer, I want errors in one symbol to not prevent processing of others so that the batch is resilient
- As a monitor, I want to track progress as symbols complete so that I can report real-time status

---

## Acceptance Scenarios

1. **Parallel Job Submission**
   - Given: ProcessingJob with multiple symbols
   - When: process_batch is called
   - Then: All symbols should be submitted to executor without waiting

2. **Result Collection**
   - Given: Multiple futures submitted to executor
   - When: Results complete in any order
   - Then: Results should be collected using as_completed()

3. **Timeout Handling**
   - Given: Symbol processing takes > 60 seconds
   - When: Timeout is triggered
   - Then: ProcessingResult with error message should be returned

4. **Exception Handling**
   - Given: Symbol processing raises exception
   - When: Exception occurs during processing
   - Then: Error should be recorded and batch continues

5. **Progress Tracking**
   - Given: progress_callback is provided
   - When: Each symbol completes
   - Then: Callback should be invoked with (completed, total)

6. **Performance Target**
   - Given: 2,500 symbols with 4 cores
   - When: Batch is processed
   - Then: Should complete in < 30 seconds

---

## Requirements

### Functional Requirements

1. Submit all job symbols to ProcessPoolExecutor in parallel
2. Collect results using `as_completed()` for efficient completion handling
3. Implement 60-second timeout per symbol processing
4. Handle `FuturesTimeoutError` with appropriate error message
5. Handle generic exceptions during symbol processing
6. Invoke progress_callback with (completed, total) for each completion
7. Log summary statistics (processed count, errors, matches, total time)
8. Support chunked submission for very large symbol lists (10,000+)

### Non-Functional Requirements

1. Performance: 2,500 symbols should process in < 30 seconds on 4 cores
2. Memory efficiency: Chunked processing for large batches
3. Error resilience: Single symbol errors should not stop batch
4. Logging: Debug and info level logging for progress tracking

---

## Key Entities

### ProcessingJob
- id: unique job identifier
- symbols: list of symbols to process
- timeframe: trading timeframe
- date_range: date range for analysis
- formula: formula to evaluate
- parameters: formula parameters

### ProcessingResult
- symbol: symbol identifier
- matches: tuple of matches found
- computed_indicators: dictionary of computed values
- processing_time_ms: execution time
- error: error message if failed

### ProcessingEngine
- _max_workers: number of worker processes
- _executor: ProcessPoolExecutor instance
- process_batch(): main parallel processing method
- process_batch_chunked(): chunked processing for large lists

---

## Dependencies

- **Depends On:** E06-F02-T02-S01 (ProcessPoolExecutor Setup)
- **Depended By:** E06-F02-T03 (Progress Tracking and Cancellation)
- **External:** Python `concurrent.futures` library

---

## Gate Checks

- [ ] ProcessPoolExecutor is properly initialized (S01 complete)
- [ ] Async context established for parallel operations
- [ ] Mock implementations for testing are available
- [ ] Performance testing environment is set up

---

## Tasks Preview

1. Implement `process_batch()` method with parallel submission
2. Implement result collection with `as_completed()`
3. Add timeout handling with 60-second limit
4. Add exception error handling
5. Implement progress callback invocation
6. Add summary logging
7. Implement optional chunked processing
8. Write performance benchmark tests
9. Write timeout handling tests
10. Write error handling tests

---

## Success Criteria

- [ ] Parallel job submission for all symbols
- [ ] `as_completed()` used for result collection
- [ ] 60-second timeout per symbol enforced
- [ ] Timeout error handling working correctly
- [ ] Exception error handling working correctly
- [ ] Progress callback invoked for each completion
- [ ] Summary logging generated
- [ ] Performance: 2,500 symbols < 30 seconds (4 cores)
- [ ] All unit tests passing
- [ ] No memory leaks in chunked processing

---

## Risk Assessment

### High Risks
1. **Timeout Duration**: 60-second timeout may be too short for some operations. Need to verify with actual data.
2. **Memory Overflow**: Large symbol lists could overwhelm executor. Chunked processing should mitigate.

### Medium Risks
1. **Error Cascading**: Errors in individual symbols handled correctly but need thorough testing.
2. **Performance Variance**: Performance depends on CPU cores available; target may vary.

### Low Risks
1. **Logging Performance**: Excessive logging could impact performance; debug logs are conditional.

### Mitigation Strategies
- Implement chunked processing with configurable chunk size
- Extensive error testing before production
- Configurable timeout parameter
- Performance monitoring and benchmarking

---

## Notes and Clarifications

### Implementation Notes

1. **Parallel Submission Strategy**: All symbols submitted upfront to maximize parallelization
2. **Result Collection**: Using `as_completed()` instead of `wait()` for efficiency
3. **Timeout Per Symbol**: Individual 60-second timeout, not batch-wide
4. **Error Recording**: Errors recorded in ProcessingResult for traceability
5. **Progress Callback**: Invoked immediately upon completion, not batched

### Chunked Processing

For very large symbol lists (10,000+), use `process_batch_chunked()` to:
- Submit symbols in chunks (default 500)
- Process each chunk separately
- Aggregate results
- Manage executor queue size

### Testing Strategy

1. **Performance Benchmark**: Test with 100 symbols, expect < 1 second with 4 workers
2. **Timeout Handling**: Mock slow process that exceeds 60-second limit
3. **Error Handling**: Mock repository that fails for specific symbols
4. **Integration**: Test with realistic data volumes

---

## Artifacts

### Code Implementation

#### Parallel Batch Processing (Main Method)

```python
from concurrent.futures import as_completed, TimeoutError as FuturesTimeoutError

class ProcessingEngine(IProcessingEngine):
    # ... existing code ...

    async def process_batch(
        self,
        job: ProcessingJob,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> list[ProcessingResult]:
        """
        Process multiple symbols in parallel.

        Submits all symbols to the process pool and collects results
        as they complete. Progress callback is invoked for each completion.

        Args:
            job: Processing job configuration
            progress_callback: Optional callback(completed, total)

        Returns:
            List of ProcessingResult for all symbols
        """
        executor = self._ensure_executor()
        results = []
        total = len(job.symbols)

        logger.info(
            f"Starting parallel batch: job_id={job.id}, "
            f"symbols={total}, workers={self._max_workers}"
        )

        # Submit all jobs to executor
        futures_map: dict[Future, str] = {}

        for symbol in job.symbols:
            future = executor.submit(
                self._process_symbol,
                symbol,
                job.timeframe,
                job.date_range,
                job.formula,
                job.parameters
            )
            futures_map[future] = symbol

        # Collect results as they complete
        completed = 0
        for future in as_completed(futures_map.keys()):
            symbol = futures_map[future]

            try:
                # Get result with timeout
                result = future.result(timeout=60)
                results.append(result)

            except FuturesTimeoutError:
                logger.error(f"Timeout processing {symbol}")
                results.append(ProcessingResult(
                    symbol=symbol,
                    matches=(),
                    computed_indicators={},
                    processing_time_ms=60000,  # 60s timeout
                    error="Processing timeout (60s)"
                ))

            except Exception as e:
                logger.error(f"Error processing {symbol}: {e}")
                results.append(ProcessingResult(
                    symbol=symbol,
                    matches=(),
                    computed_indicators={},
                    processing_time_ms=0,
                    error=str(e)
                ))

            completed += 1

            # Progress callback
            if progress_callback:
                progress_callback(completed, total)

            # Debug logging
            if completed % 100 == 0 or completed == total:
                logger.debug(f"Progress: {completed}/{total}")

        # Summary
        errors = sum(1 for r in results if r.error)
        total_matches = sum(len(r.matches) for r in results)
        total_time = sum(r.processing_time_ms for r in results)

        logger.info(
            f"Parallel batch complete: job_id={job.id}, "
            f"processed={total}, errors={errors}, "
            f"matches={total_matches}, total_time={total_time:.0f}ms"
        )

        return results
```

#### Chunked Submission (Optional Optimization)

For very large symbol lists, submit in chunks to manage memory:

```python
async def process_batch_chunked(
    self,
    job: ProcessingJob,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    chunk_size: int = 500
) -> list[ProcessingResult]:
    """
    Process symbols in chunks for memory efficiency.

    For very large symbol lists (10,000+), processes in chunks
    to avoid overwhelming the process pool.
    """
    all_results = []
    total = len(job.symbols)
    completed = 0

    for i in range(0, total, chunk_size):
        chunk_symbols = job.symbols[i:i + chunk_size]

        # Create sub-job for chunk
        chunk_job = ProcessingJob(
            id=f"{job.id}-chunk-{i}",
            job_type=job.job_type,
            symbols=chunk_symbols,
            timeframe=job.timeframe,
            date_range=job.date_range,
            formula=job.formula,
            parameters=job.parameters
        )

        # Process chunk
        def chunk_progress(c: int, t: int):
            if progress_callback:
                progress_callback(completed + c, total)

        chunk_results = await self._process_batch_parallel(
            chunk_job,
            chunk_progress
        )

        all_results.extend(chunk_results)
        completed += len(chunk_symbols)

    return all_results
```

### Test Cases

```python
@pytest.mark.benchmark
async def test_parallel_performance():
    """Benchmark parallel processing performance"""
    # Create mock that simulates 10ms per symbol
    mock_repo = MagicMock()
    mock_repo.get_range.side_effect = lambda **kw: (
        time.sleep(0.01),  # 10ms
        [mock_candle()]
    )[1]

    mock_service = MagicMock()
    mock_service.evaluate.return_value = pd.Series([False])

    with ProcessingEngine(mock_repo, mock_service, max_workers=4) as engine:
        job = ProcessingJob(
            id="benchmark",
            job_type=JobType.SCAN,
            symbols=[f"SYM{i:04d}" for i in range(100)],
            timeframe="1d",
            date_range=DateRange(date(2024, 1, 1), date(2024, 12, 31))
        )

        start = time.time()
        results = await engine.process_batch(job)
        elapsed = time.time() - start

    assert len(results) == 100
    # With 4 workers, should be ~4x faster than sequential
    # 100 symbols * 10ms / 4 workers = ~250ms
    assert elapsed < 1.0  # Allow generous margin

async def test_parallel_timeout_handling():
    """Test timeout handling for slow symbols"""
    def slow_process(*args, **kwargs):
        time.sleep(120)  # Exceeds 60s timeout
        return mock_result()

    with patch.object(ProcessingEngine, '_process_symbol', slow_process):
        engine = ProcessingEngine(MagicMock(), MagicMock(), max_workers=1)

        job = ProcessingJob(
            id="timeout-test",
            job_type=JobType.SCAN,
            symbols=["SLOW"],
            timeframe="1d",
            date_range=DateRange(date(2024, 1, 1), date(2024, 12, 31))
        )

        results = await engine.process_batch(job)

        assert len(results) == 1
        assert "timeout" in results[0].error.lower()

async def test_parallel_error_handling():
    """Test error handling doesn't stop batch"""
    mock_repo = MagicMock()
    mock_repo.get_range.side_effect = [
        [mock_candle()],
        Exception("Simulated error"),
        [mock_candle()],
    ]

    with ProcessingEngine(mock_repo, MagicMock(), max_workers=2) as engine:
        job = ProcessingJob(
            id="error-test",
            job_type=JobType.SCAN,
            symbols=["A", "B", "C"],
            timeframe="1d",
            date_range=DateRange(date(2024, 1, 1), date(2024, 12, 31))
        )

        results = await engine.process_batch(job)

    assert len(results) == 3
    errors = [r for r in results if r.error]
    assert len(errors) == 1
```

---

## Metadata

| Field | Value |
|-------|-------|
| Subtask ID | E06-F02-T02-S02 |
| Title | Parallel Batch Execution |
| Status | Draft |
| Parent Task | E06-F02-T02 |
| Effort | M (Medium) |
| Dependencies | E06-F02-T02-S01 |
| Created | TBD |
| Last Updated | 2025-12-28 |
