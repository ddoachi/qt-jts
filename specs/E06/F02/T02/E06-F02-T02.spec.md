# Spec: E06-F02-T02 - Implement Parallel Processing with ProcessPool

---

## Frontmatter

```yaml
id: E06-F02-T02
clickup_task_id: null
title: Implement Parallel Processing with ProcessPool
type: task
parent: E06-F02
children:
  - E06-F02-T02-S01
  - E06-F02-T02-S02
epic: E06
feature: F02
task: T02
domain: processing-engine
status: Draft
priority: High
dates:
  start_date: null
  target_date: null
hours: 40
tags:
  - parallel-processing
  - performance
  - cpu-bound
effort: L
risk: Medium
```

---

## Status

**Current Status:** Draft | **Effort:** L (Large) | **Risk:** Medium

---

## Executive Summary

Enhance the ProcessingEngine with parallel execution using Python's `ProcessPoolExecutor`. This enables processing multiple symbols concurrently across CPU cores for significant performance gains. The implementation will support configurable worker pool size, graceful error handling, and result collection using `as_completed()` for optimal performance.

---

## Execution Flow

1. Initialize ProcessPoolExecutor context manager with configurable max_workers
2. Submit all symbol processing jobs to the worker pool
3. Collect results as they complete using `as_completed()`
4. Handle timeouts and exceptions per worker task
5. Provide progress callbacks for job monitoring
6. Gracefully shutdown executor on completion or error

---

## User Stories

- As a developer, I want to process multiple symbols in parallel so that I can reduce total processing time
- As an operations engineer, I want to configure the worker pool size so that I can optimize resource usage
- As a system monitor, I want progress callbacks so that I can track long-running batch operations
- As a reliability engineer, I want proper error handling so that individual symbol failures don't crash the entire batch

---

## Acceptance Scenarios

| Scenario | Expected Outcome |
|----------|------------------|
| Process 2,500 symbols with 4 cores | Complete in < 30 seconds |
| A worker process times out after 60s | Result recorded with error; batch continues |
| A worker process raises an exception | Exception caught; error recorded; batch continues |
| ProcessPoolExecutor lifecycle management | Executor created on context entry; shutdown on exit |
| Progress callback invocation | Callback fires for each completed symbol |
| Configurable max_workers | Can set custom worker count; defaults to CPU count |

---

## Requirements

### Functional Requirements

1. **ProcessPoolExecutor Integration**
   - Integrate ProcessPoolExecutor for parallel symbol processing
   - Support context manager protocol (`__enter__`, `__exit__`)
   - Default max_workers to `os.cpu_count()` or 4

2. **Parallel Job Execution**
   - Submit all symbols to worker pool without blocking
   - Use `as_completed()` for efficient result collection
   - Process each symbol independently with its own parameters

3. **Error and Timeout Handling**
   - Implement 60-second timeout per symbol
   - Catch TimeoutError and record with error message
   - Catch generic exceptions and record with error details
   - Ensure errors don't halt the entire batch

4. **Progress Tracking**
   - Support optional progress_callback parameter
   - Invoke callback with (completed_count, total_count) on each completion
   - Enable real-time monitoring of batch progress

### Non-Functional Requirements

1. **Performance**
   - Process 2,500 symbols in less than 30 seconds with 4 cores
   - Minimize GIL contention through true parallelism

2. **Reliability**
   - Graceful degradation on worker failures
   - Proper resource cleanup on shutdown

3. **Configurability**
   - Allow max_workers override at initialization
   - Support lazy executor creation

---

## Key Entities

### ProcessingEngine

```python
class ProcessingEngine(IProcessingEngine):
    """Core processing engine with parallel execution"""

    def __init__(
        self,
        candle_repo: ICandleRepository,
        formula_service: FormulaService,
        max_workers: int = None
    ):
        self._candle_repo = candle_repo
        self._formula_service = formula_service
        self._max_workers = max_workers or (os.cpu_count() or 4)
        self._executor: Optional[ProcessPoolExecutor] = None

    def __enter__(self):
        self._executor = ProcessPoolExecutor(max_workers=self._max_workers)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._executor:
            self._executor.shutdown(wait=True)
        return False

    async def process_batch(
        self,
        job: ProcessingJob,
        progress_callback: Callable[[int, int], None] = None
    ) -> list[ProcessingResult]:
        """Process multiple symbols in parallel"""
        results = []
        total = len(job.symbols)

        if not self._executor:
            self._executor = ProcessPoolExecutor(max_workers=self._max_workers)

        # Submit all jobs
        futures = {}
        for symbol in job.symbols:
            future = self._executor.submit(
                self._process_symbol,
                symbol,
                job.timeframe,
                job.date_range,
                job.formula,
                job.parameters
            )
            futures[future] = symbol

        # Collect results as they complete
        completed = 0
        for future in as_completed(futures):
            symbol = futures[future]
            try:
                result = future.result(timeout=60)
                results.append(result)
            except TimeoutError:
                results.append(ProcessingResult(
                    symbol=symbol,
                    matches=(),
                    computed_indicators={},
                    processing_time_ms=0,
                    error="Processing timeout"
                ))
            except Exception as e:
                results.append(ProcessingResult(
                    symbol=symbol,
                    matches=(),
                    computed_indicators={},
                    processing_time_ms=0,
                    error=str(e)
                ))

            completed += 1
            if progress_callback:
                progress_callback(completed, total)

        return results
```

### ProcessingResult

Result entity containing symbol, matches, computed indicators, processing time, and error message if applicable.

### ProcessingJob

Job entity containing symbols, timeframe, date_range, formula, and parameters for batch processing.

---

## Dependencies

| Dependency | Type | Reference | Notes |
|-----------|------|-----------|-------|
| E06-F02-T01 | Parent Task | ProcessingEngine Core | Must implement core engine before parallel features |
| Python ProcessPoolExecutor | Library | Standard Library | Built-in concurrent.futures module |
| ICandleRepository | Interface | E06 | Required for data retrieval in workers |
| FormulaService | Service | E06 | Required for formula computation in workers |

---

## Gate Checks

- [ ] E06-F02-T01 (ProcessingEngine Core) is completed
- [ ] All subtasks (S01, S02) are defined and scoped
- [ ] Performance target (2,500 symbols < 30 seconds) is feasible with target hardware
- [ ] Serialization approach is verified for all job parameters
- [ ] Error handling strategy approved by team

---

## Tasks Preview

### Subtasks

This task is decomposed into the following subtasks:

| Subtask | Title | Description | Status |
|---------|-------|-------------|--------|
| E06-F02-T02-S01 | ProcessPoolExecutor setup | Worker pool initialization and management | Pending |
| E06-F02-T02-S02 | Parallel batch execution | Concurrent symbol processing and result collection | Pending |

---

## Success Criteria

### Implementation Success

- [ ] ProcessPoolExecutor integrated into ProcessingEngine
- [ ] Context manager pattern implemented for lifecycle management
- [ ] All symbols submitted to worker pool asynchronously
- [ ] Results collected using `as_completed()` pattern
- [ ] 60-second timeout per symbol enforced
- [ ] Worker exceptions caught and recorded
- [ ] max_workers configurable with sensible defaults
- [ ] Progress callback mechanism working
- [ ] Code follows existing codebase patterns

### Performance Success

- [ ] 2,500 symbol batch completes in < 30 seconds with 4 cores
- [ ] Demonstrates linear scalability with core count (up to reasonable limit)
- [ ] No memory leaks in long-running operations

### Testing Success

- [ ] Unit tests for ProcessPoolExecutor initialization
- [ ] Unit tests for context manager lifecycle
- [ ] Unit tests for error handling scenarios
- [ ] Unit tests for timeout handling
- [ ] Integration tests for full batch processing
- [ ] Performance benchmark tests
- [ ] All tests pass with 100% success rate

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|-----------|
| Serialization issues with pickling | Medium | High | Pre-validate all job parameters are picklable; implement _prepare_for_worker() |
| Worker process deadlocks | Low | High | Implement watchdog timeout; proper resource cleanup in __exit__ |
| Memory explosion with large batches | Medium | Medium | Monitor worker memory; implement backpressure or chunking if needed |
| Performance doesn't meet target | Medium | High | Benchmark early; profile hotspots; consider C extensions if needed |
| OS process limit exceeded | Low | Medium | Validate max_workers against system limits; warn if too high |

---

## Serialization Considerations

For ProcessPoolExecutor, objects must be picklable. Implement preparation step:

```python
def _prepare_for_worker(self, job: ProcessingJob) -> dict:
    """Prepare job data for worker process"""
    return {
        'symbols': job.symbols,
        'timeframe': job.timeframe,
        'date_range': (job.date_range.start, job.date_range.end),
        'formula_expression': job.formula.expression if job.formula else None,
        'parameters': job.parameters
    }
```

---

## Notes and Clarifications

1. **Lazy Executor Initialization**: The executor can be created on first use if not created during `__enter__`. This provides flexibility for different usage patterns.

2. **Async/Await Compatibility**: The `process_batch` method is async to maintain compatibility with existing async interfaces, even though ProcessPoolExecutor itself is synchronous.

3. **Result Ordering**: Results are returned in completion order (not submission order). Clients should not rely on result ordering matching symbol order.

4. **Worker Isolation**: Each worker process is independent. Shared state must be passed via job parameters; no global state is accessible in workers.

5. **Timeout Granularity**: The 60-second timeout applies to individual `future.result()` calls, not total batch time.

---

## Testing Requirements

### Unit Tests

```python
@pytest.mark.asyncio
async def test_executor_initialization():
    """Test ProcessPoolExecutor context manager"""
    engine = ProcessingEngine(mock_repo, mock_service, max_workers=2)

    assert engine._executor is None
    with engine:
        assert engine._executor is not None
        assert engine._executor._max_workers == 2
    assert engine._executor is None

@pytest.mark.asyncio
async def test_error_handling():
    """Test exception handling in worker processes"""
    engine = ProcessingEngine(mock_repo_with_errors, mock_service)

    job = ProcessingJob(
        id="error-test",
        symbols=["BAD_SYMBOL_1", "BAD_SYMBOL_2"],
        timeframe="1d",
        date_range=DateRange(date(2024, 1, 1), date(2024, 1, 31))
    )

    with engine:
        results = await engine.process_batch(job)

    assert len(results) == 2
    assert all(r.error is not None for r in results)

@pytest.mark.asyncio
async def test_timeout_handling():
    """Test timeout handling for slow workers"""
    engine = ProcessingEngine(mock_repo_slow, mock_service)

    job = ProcessingJob(
        id="timeout-test",
        symbols=["SLOW_SYMBOL"],
        timeframe="1d",
        date_range=DateRange(date(2024, 1, 1), date(2024, 1, 31))
    )

    with engine:
        results = await engine.process_batch(job)

    assert len(results) == 1
    assert results[0].error == "Processing timeout"

@pytest.mark.asyncio
async def test_progress_callback():
    """Test progress callback invocation"""
    engine = ProcessingEngine(mock_repo, mock_service)
    progress_updates = []

    def progress_callback(completed, total):
        progress_updates.append((completed, total))

    job = ProcessingJob(
        id="progress-test",
        symbols=["SYM1", "SYM2", "SYM3"],
        timeframe="1d",
        date_range=DateRange(date(2024, 1, 1), date(2024, 1, 31))
    )

    with engine:
        await engine.process_batch(job, progress_callback)

    assert len(progress_updates) == 3
    assert progress_updates[-1] == (3, 3)
```

### Performance Tests

```python
@pytest.mark.benchmark
async def test_parallel_processing_performance():
    """Benchmark parallel processing with 2,500 symbols"""
    engine = ProcessingEngine(mock_repo, mock_service, max_workers=4)

    job = ProcessingJob(
        id="benchmark",
        job_type=JobType.SCAN,
        symbols=[f"SYM{i:04d}" for i in range(2500)],
        timeframe="1d",
        date_range=DateRange(date(2024, 1, 1), date(2024, 12, 31))
    )

    start = time.time()
    with engine:
        results = await engine.process_batch(job)
    elapsed = time.time() - start

    assert elapsed < 30  # Must complete in < 30 seconds
    assert len(results) == 2500
```

---

## Artifacts

- **Source Code**: ProcessingEngine parallel implementation
- **Tests**: Unit and performance test suite
- **Documentation**: Docstrings, type hints, usage examples
- **Benchmarks**: Performance comparison reports (serial vs. parallel)

---

## Metadata

| Field | Value |
|-------|-------|
| Task ID | E06-F02-T02 |
| Title | Implement Parallel Processing with ProcessPool |
| Status | Draft |
| Feature | E06-F02: Processing Engine Core |
| Effort | L (Large) |
| Hours Estimated | 40 |
| Dependencies | E06-F02-T01 |
| Created | 2025-12-28 |
| Last Updated | 2025-12-28 |

---

## References

- E06.spec.md Section 4.1: Core Engine
- E06-F02-T01: ProcessingEngine Core
- Python ProcessPoolExecutor documentation: https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor
- concurrent.futures.as_completed documentation: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.as_completed
