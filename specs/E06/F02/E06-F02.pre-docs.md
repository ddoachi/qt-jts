# E06-F02: Processing Engine Core - Pre-Implementation Planning

## Document Information

| Field | Value |
|-------|-------|
| Feature ID | E06-F02 |
| Title | Processing Engine Core |
| Version | 1.0 |
| Date | 2025-12-28 |
| Status | Ready for Implementation |

---

## 1. Overview and Objectives

### 1.1 Purpose

Implement the core processing engine that orchestrates high-performance batch processing of formulas across multiple symbols. This engine serves as the foundation for the Scanner, Backtesting, and Trading modules by providing a unified, parallelized computation framework.

### 1.2 Key Objectives

1. **High Performance**: Process 2,500 symbols in under 30 seconds on 4-core systems
2. **Parallel Execution**: Leverage `ProcessPoolExecutor` for efficient multi-core utilization
3. **Graceful Error Handling**: Handle missing data and processing failures without crashing
4. **Progress Tracking**: Provide real-time progress feedback with cancellation support
5. **Memory Efficiency**: Maintain memory usage below 2GB during batch processing

### 1.3 Success Criteria

- Single symbol processing completes in under 50ms
- Parallel execution scales linearly up to the number of available CPU cores
- Zero data loss during error scenarios
- All processing errors are logged and returned in results
- Progress callbacks fire with accurate completion percentages

---

## 2. Technical Approach

### 2.1 Architecture Pattern

The Processing Engine follows a **Pipeline Architecture** with **Worker Pool Pattern** for parallelization:

```
Input (ProcessingJob)
  -> Symbol Distribution (Queue)
    -> Worker Pool (ProcessPoolExecutor)
      -> Per-Symbol Pipeline:
         1. Data Loader (load OHLCV from repository)
         2. DataFrame Preparation (convert to pandas)
         3. Formula Evaluator (DSL expression evaluation)
         4. Signal Emitter (create SignalMatch objects)
      -> Results Collection
    -> Aggregated Results (ProcessingResult list)
```

### 2.2 Concurrency Model

- **Multi-Process Parallelism**: Use `ProcessPoolExecutor` to bypass Python's GIL
- **Process Pool Size**: Default to `os.cpu_count()` or 4 workers minimum
- **Task Distribution**: Each symbol becomes an independent task in the worker pool
- **Result Collection**: Gather results as futures complete with timeout protection

### 2.3 Data Flow

```
ProcessingJob (symbols, formula, date_range, timeframe)
   |
   +-> For each symbol:
   |     |
   |     +-> Load candles from ICandleRepository
   |     +-> Convert to pandas DataFrame
   |     +-> Evaluate formula using FormulaService
   |     +-> Extract matches where formula returns True
   |     +-> Create ProcessingResult with timing info
   |
   +-> Collect all ProcessingResults
   +-> Return to caller
```

### 2.4 Error Handling Strategy

1. **Per-Symbol Isolation**: Errors in one symbol do not affect others
2. **Graceful Degradation**: Return `ProcessingResult` with error field populated
3. **Timeout Protection**: 60-second timeout per symbol to prevent hung workers
4. **Data Validation**: Check for empty candle data before processing
5. **Exception Capture**: Wrap all worker operations in try-except blocks

---

## 3. Dependencies and Prerequisites

### 3.1 Direct Dependencies

| Dependency | Type | Reason |
|------------|------|--------|
| E06-F01 (Domain Model) | Required | Provides `ProcessingJob`, `ProcessingResult`, `SignalMatch` entities |
| E05 (DSL Parser) | Required | Provides `FormulaService` for expression evaluation |
| E02 (Storage Layer) | Required | Provides `ICandleRepository` for data access |

### 3.2 Python Standard Library

- `concurrent.futures.ProcessPoolExecutor`: Multi-process parallelization
- `time.perf_counter()`: High-resolution timing
- `os.cpu_count()`: CPU core detection
- `dataclasses.asdict()`: Entity to dict conversion

### 3.3 Third-Party Libraries

- `pandas`: DataFrame operations for time-series data
- `typing`: Type hints for interfaces

### 3.4 Prerequisites

1. Domain model entities must be defined and serializable (for inter-process communication)
2. `ICandleRepository` interface must be implemented
3. `FormulaService` must support DataFrame-based evaluation
4. Test data generator for candles must be available

---

## 4. Implementation Plan

### 4.1 Task Breakdown

#### Task E06-F02-T01: Implement ProcessingEngine Core (Large)

**Subtask 1: Create ProcessingEngine class skeleton**
- Define `__init__` method accepting repositories and configuration
- Set up `_max_workers` based on CPU cores
- Initialize `ProcessPoolExecutor` in constructor
- Add cleanup method for executor shutdown

**Subtask 2: Implement `_process_symbol` worker function**
- Load candles for symbol using `ICandleRepository`
- Handle empty data case with proper error result
- Convert candles to pandas DataFrame
- Add timing instrumentation
- Return `ProcessingResult` with all required fields

**Subtask 3: Implement formula evaluation logic**
- Call `FormulaService.evaluate()` with DataFrame
- Extract boolean result series
- Create `SignalMatch` for each True value
- Extract indicator values at match timestamps
- Handle evaluation errors gracefully

#### Task E06-F02-T02: Implement Parallel Processing (Large)

**Subtask 1: Implement `process_batch` method**
- Accept `ProcessingJob` and optional progress callback
- Submit all symbols to executor as futures
- Track future-to-symbol mapping
- Implement results collection loop

**Subtask 2: Add timeout and error handling**
- Set 60-second timeout on future.result()
- Catch timeout exceptions and create error result
- Catch all other exceptions with proper logging
- Ensure all symbols return a result (success or error)

#### Task E06-F02-T03: Add Progress Tracking and Cancellation (Medium)

**Subtask 1: Implement progress callback mechanism**
- Fire callback after each symbol completes
- Pass `(completed, total)` to callback function
- Handle callback exceptions gracefully

**Subtask 2: Add cancellation support**
- Add `_cancelled` flag to engine
- Check flag in processing loop
- Implement `cancel()` method to set flag
- Cancel pending futures when cancellation requested

### 4.2 Implementation Order

1. **Phase 1**: Core single-symbol processing
   - Implement `_process_symbol` method
   - Test with in-memory repository and mock formula
   - Verify DataFrame conversion and result creation

2. **Phase 2**: Parallel batch processing
   - Implement `process_batch` with ProcessPoolExecutor
   - Test with 2-3 symbols initially
   - Verify result collection and error handling

3. **Phase 3**: Progress and cancellation
   - Add progress callback support
   - Implement cancellation mechanism
   - Test interruption scenarios

4. **Phase 4**: Performance tuning
   - Benchmark with 100, 500, 1000, 2500 symbols
   - Optimize DataFrame operations
   - Tune worker pool size

---

## 5. File Structure and Locations

### 5.1 Primary Implementation Files

```
src/qt_jts/
├── processing/
│   ├── __init__.py
│   ├── engine.py                    # ProcessingEngine class (NEW)
│   ├── models.py                    # Import from domain (existing via F01)
│   └── exceptions.py                # Processing-specific exceptions (NEW)
```

### 5.2 Test Files

```
tests/
├── unit/
│   └── processing/
│       ├── __init__.py
│       ├── test_engine.py           # ProcessingEngine unit tests (NEW)
│       └── test_engine_parallel.py  # Parallelization tests (NEW)
├── integration/
│   └── processing/
│       ├── __init__.py
│       └── test_engine_integration.py  # End-to-end tests (NEW)
└── performance/
    └── test_engine_benchmark.py     # Performance benchmarks (NEW)
```

### 5.3 Supporting Files

```
src/qt_jts/
├── domain/
│   └── models.py                    # ProcessingJob, ProcessingResult (from F01)
├── storage/
│   └── repositories.py              # ICandleRepository (from E02)
└── dsl/
    └── evaluator.py                 # FormulaService (from E05)
```

---

## 6. Key Interfaces and Contracts

### 6.1 ProcessingEngine Public API

```python
class ProcessingEngine:
    """Core processing engine for batch formula evaluation"""

    def __init__(
        self,
        candle_repo: ICandleRepository,
        formula_service: FormulaService,
        max_workers: int = None
    ):
        """
        Initialize processing engine

        Args:
            candle_repo: Repository for loading candle data
            formula_service: Service for evaluating formulas
            max_workers: Number of worker processes (default: CPU count)
        """
        pass

    async def process_batch(
        self,
        job: ProcessingJob,
        progress_callback: Callable[[int, int], None] = None
    ) -> list[ProcessingResult]:
        """
        Process multiple symbols in parallel

        Args:
            job: Processing job with symbols and formula
            progress_callback: Optional callback(completed, total)

        Returns:
            List of ProcessingResult (one per symbol)

        Raises:
            ProcessingEngineError: On critical failures
        """
        pass

    def cancel(self) -> None:
        """Request cancellation of current batch processing"""
        pass

    def shutdown(self, wait: bool = True) -> None:
        """Shutdown the executor and cleanup resources"""
        pass
```

### 6.2 Worker Function Contract

```python
def _process_symbol(
    symbol: str,
    timeframe: str,
    date_range: DateRange,
    formula: Optional[Formula],
    parameters: dict[str, Any],
    candle_repo: ICandleRepository,
    formula_service: FormulaService
) -> ProcessingResult:
    """
    Process a single symbol (runs in worker process)

    Args:
        symbol: Symbol code to process
        timeframe: Candle timeframe (e.g., "1d")
        date_range: Date range for data loading
        formula: Optional formula to evaluate
        parameters: Additional processing parameters
        candle_repo: Repository instance (must be picklable)
        formula_service: Formula evaluator instance

    Returns:
        ProcessingResult with matches and timing info

    Note: This function must be picklable for ProcessPoolExecutor
    """
    pass
```

### 6.3 Repository Contract Requirements

```python
class ICandleRepository(Protocol):
    """Required interface for candle data access"""

    def get_range(
        self,
        symbol_id: int,
        timeframe: str,
        start: date,
        end: date
    ) -> list[Candle]:
        """
        Load candles for date range

        Returns:
            List of Candle objects (may be empty)
        """
        pass
```

### 6.4 FormulaService Contract Requirements

```python
class FormulaService:
    """Required interface for formula evaluation"""

    def evaluate(
        self,
        formula: Formula,
        df: pd.DataFrame,
        parameters: dict[str, Any] = None
    ) -> pd.Series:
        """
        Evaluate formula against DataFrame

        Args:
            formula: Formula object with expression
            df: DataFrame with OHLCV columns and datetime index
            parameters: Optional runtime parameters

        Returns:
            Boolean Series indicating matches
        """
        pass
```

---

## 7. Testing Strategy

### 7.1 Unit Tests

**Test Coverage Requirements**: > 85%

**Core Test Cases**:

1. **Single Symbol Processing**
   - Test successful processing with valid data
   - Test empty data handling
   - Test formula evaluation with matches
   - Test formula evaluation with no matches
   - Test timing measurement accuracy

2. **Batch Processing**
   - Test processing 2-10 symbols
   - Test mixed success/failure scenarios
   - Test all symbols succeed
   - Test all symbols fail

3. **Error Handling**
   - Test missing data gracefully returns error result
   - Test formula evaluation errors are caught
   - Test timeout handling (with slow mock)
   - Test invalid symbol handling

4. **Progress Tracking**
   - Test progress callback fires correctly
   - Test progress counts are accurate
   - Test callback exceptions don't break processing

5. **Cancellation**
   - Test cancel() stops processing
   - Test partial results returned on cancellation
   - Test cancellation mid-batch

### 7.2 Integration Tests

**Test Cases**:

1. **End-to-End Workflow**
   - Create real ProcessingJob
   - Load actual test data from repository
   - Evaluate real formula
   - Verify SignalMatch objects created correctly

2. **Multi-Symbol Integration**
   - Process 50-100 symbols with real data
   - Verify all results returned
   - Verify no memory leaks
   - Verify executor cleanup

3. **Repository Integration**
   - Test with InMemoryRepository
   - Test with database-backed repository
   - Verify data loading performance

### 7.3 Performance Tests

**Benchmark Requirements**:

| Test Case | Symbols | Expected Time | Max Memory |
|-----------|---------|---------------|------------|
| Small batch | 10 | < 1 second | < 100 MB |
| Medium batch | 100 | < 5 seconds | < 500 MB |
| Large batch | 500 | < 15 seconds | < 1 GB |
| Full batch | 2,500 | < 30 seconds | < 2 GB |

**Performance Test Cases**:

1. **Scalability Test**
   - Process with 1, 2, 4, 8 workers
   - Verify linear scaling up to CPU count
   - Measure speedup factor

2. **Memory Test**
   - Process 2,500 symbols
   - Monitor memory usage throughout
   - Verify no memory leaks after completion

3. **Single Symbol Latency**
   - Process one symbol repeatedly
   - Measure p50, p95, p99 latency
   - Verify < 50ms average

### 7.4 Test Data Requirements

**Mock Data Generators**:

```python
def generate_test_candles(count: int, start_date: date = None) -> list[Candle]:
    """Generate realistic test candle data"""
    pass

def generate_test_symbols(count: int) -> list[str]:
    """Generate test symbol codes"""
    pass

def create_test_formula(expression: str) -> Formula:
    """Create test formula object"""
    pass
```

**Test Fixtures**:

- In-memory candle repository with pre-populated data
- Mock formula service with configurable responses
- Slow formula service for timeout testing
- Progress callback spy for verification

---

## 8. Risks and Mitigations

### 8.1 Performance Risks

**Risk P1: Processing time exceeds 30 seconds for 2,500 symbols**

- **Likelihood**: Medium
- **Impact**: High (blocks acceptance criteria)
- **Mitigation**:
  - Profile single-symbol processing first
  - Optimize DataFrame operations (avoid repeated conversions)
  - Consider pre-loading all data before parallel processing
  - Use pandas vectorization instead of row iteration
  - Benchmark early and often

**Risk P2: Memory usage exceeds 2GB**

- **Likelihood**: Medium
- **Impact**: High (system crashes on limited hardware)
- **Mitigation**:
  - Process in smaller batches if needed
  - Release DataFrames after processing each symbol
  - Monitor memory in tests
  - Implement memory limits in executor

### 8.2 Concurrency Risks

**Risk C1: ProcessPoolExecutor pickling issues**

- **Likelihood**: High
- **Impact**: Critical (blocks implementation)
- **Mitigation**:
  - Ensure all objects are picklable (dataclasses, simple types)
  - Avoid lambda functions in worker code
  - Test serialization early
  - Use `multiprocessing.get_context('spawn')` if needed

**Risk C2: Worker process hangs or crashes**

- **Likelihood**: Medium
- **Impact**: High (stalls entire batch)
- **Mitigation**:
  - Implement 60-second timeout per symbol
  - Add worker health checks
  - Log worker exceptions thoroughly
  - Implement retry logic for transient failures

**Risk C3: Race conditions in progress tracking**

- **Likelihood**: Low
- **Impact**: Medium (incorrect progress display)
- **Mitigation**:
  - Use atomic counters
  - Test concurrent callback execution
  - Keep callback logic minimal

### 8.3 Data Risks

**Risk D1: Missing or incomplete candle data**

- **Likelihood**: High
- **Impact**: Medium (some symbols fail)
- **Mitigation**:
  - Validate data before processing
  - Return clear error messages
  - Log missing data issues
  - Continue processing other symbols

**Risk D2: DataFrame conversion overhead**

- **Likelihood**: Medium
- **Impact**: Medium (slows processing)
- **Mitigation**:
  - Optimize conversion logic
  - Consider caching DataFrames (future enhancement)
  - Use efficient pandas constructors
  - Profile conversion time separately

### 8.4 Integration Risks

**Risk I1: FormulaService evaluation errors**

- **Likelihood**: Medium
- **Impact**: High (no matches found)
- **Mitigation**:
  - Validate formula before batch processing
  - Wrap evaluation in try-except
  - Return detailed error information
  - Test with various formula types

**Risk I2: Repository timeout or connection issues**

- **Likelihood**: Medium (if using database)
- **Impact**: High (all symbols fail)
- **Mitigation**:
  - Add connection retry logic
  - Validate repository health before batch
  - Consider connection pooling
  - Fail fast with clear error message

### 8.5 Testing Risks

**Risk T1: Insufficient test coverage**

- **Likelihood**: Medium
- **Impact**: High (bugs in production)
- **Mitigation**:
  - Enforce 85% coverage requirement
  - Write tests before implementation (TDD)
  - Review coverage reports
  - Add integration tests for edge cases

**Risk T2: Performance tests unstable on CI**

- **Likelihood**: High
- **Impact**: Medium (flaky builds)
- **Mitigation**:
  - Use relative performance metrics
  - Run performance tests on dedicated hardware
  - Add tolerance to timing assertions
  - Skip performance tests on resource-constrained CI

---

## 9. Implementation Checklist

### Phase 1: Core Implementation

- [ ] Create `src/qt_jts/processing/engine.py`
- [ ] Define `ProcessingEngine` class with `__init__`
- [ ] Implement `_process_symbol` worker function
- [ ] Add DataFrame conversion logic
- [ ] Implement formula evaluation integration
- [ ] Add timing instrumentation
- [ ] Write unit tests for single-symbol processing

### Phase 2: Parallel Processing

- [ ] Initialize `ProcessPoolExecutor` in constructor
- [ ] Implement `process_batch` method
- [ ] Add future submission loop
- [ ] Add result collection with timeout
- [ ] Implement error handling for each symbol
- [ ] Write tests for batch processing
- [ ] Test with 2-10 symbols

### Phase 3: Progress and Cancellation

- [ ] Add `progress_callback` parameter
- [ ] Fire callback after each completion
- [ ] Implement cancellation flag
- [ ] Add `cancel()` method
- [ ] Test progress tracking
- [ ] Test cancellation mid-batch

### Phase 4: Testing and Validation

- [ ] Write integration tests
- [ ] Create performance benchmarks
- [ ] Run benchmark with 2,500 symbols
- [ ] Verify < 30 second processing time
- [ ] Verify < 2GB memory usage
- [ ] Verify > 85% test coverage

### Phase 5: Documentation and Cleanup

- [ ] Add docstrings to all public methods
- [ ] Document performance characteristics
- [ ] Add usage examples in docstrings
- [ ] Update epic documentation with actual performance
- [ ] Create troubleshooting guide

---

## 10. Performance Targets

### 10.1 Latency Targets

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| Single symbol | < 50ms | `time.perf_counter()` |
| 10 symbols | < 500ms | Total batch time |
| 100 symbols | < 5s | Total batch time |
| 2,500 symbols | < 30s | Total batch time |

### 10.2 Throughput Targets

| Workers | Symbols/sec | Efficiency |
|---------|-------------|-----------|
| 1 core | ~20 | Baseline |
| 2 cores | ~40 | 100% |
| 4 cores | ~80+ | 100% |
| 8 cores | ~150+ | 95% |

### 10.3 Resource Targets

| Resource | Limit | Monitoring |
|----------|-------|------------|
| Memory | < 2GB | `psutil.Process().memory_info()` |
| CPU | ~100% per core | Top/htop during batch |
| File handles | < 100 | `lsof` count |

---

## 11. Future Enhancements (Out of Scope)

The following enhancements are identified but not part of this feature:

1. **Indicator Caching** (E06-F05)
   - Cache computed indicators across symbols
   - LRU eviction policy
   - Memory-aware caching

2. **Signal Aggregation** (E06-F04)
   - Group matches by date
   - Statistical analysis
   - Ranking and filtering

3. **Streaming Processing** (E06-F06)
   - Real-time candle processing
   - Incremental indicator updates
   - WebSocket integration

4. **Advanced Optimizations**
   - Batch data loading (load all symbols at once)
   - Shared memory for DataFrames
   - Numba JIT compilation for indicators
   - GPU acceleration for large batches

---

## 12. References

### 12.1 Specification Documents

- `/specs/E06/F02/E06-F02.spec.md` - Feature specification
- `/specs/E06/E06.spec.md` - Epic specification
- `/specs/E06/F01/E06-F01.spec.md` - Domain model specification

### 12.2 Python Documentation

- [concurrent.futures.ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor)
- [multiprocessing - Process-based parallelism](https://docs.python.org/3/library/multiprocessing.html)
- [pandas Performance Tips](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)

### 12.3 Design Patterns

- Pipeline Pattern for data processing flow
- Worker Pool Pattern for parallel execution
- Repository Pattern for data access abstraction
- Service Pattern for formula evaluation

---

## Document Approval

This pre-implementation document should be reviewed and approved before implementation begins.

**Prepared by**: Architecture Team
**Date**: 2025-12-28
**Status**: Ready for Review

**Review Checklist**:
- [ ] All dependencies identified and available
- [ ] Technical approach validated
- [ ] Performance targets realistic
- [ ] Risks adequately mitigated
- [ ] Test strategy comprehensive
- [ ] Implementation plan clear and actionable
