# Spec: E06-F02-T01-S02 - Single Symbol Processing

---

id: E06-F02-T01-S02
clickup_task_id: null
title: Single Symbol Processing
type: subtask
parent: E06-F02-T01
children: []
epic: E06
feature: F02
task: T01
subtask: S02
domain: processing-engine
status: Draft
priority: Medium
dates:
  created: null
  started: null
  completed: null
  deadline: null
hours:
  estimated: null
  actual: null
tags:
  - processing-engine
  - core-implementation
  - single-symbol
effort: M
risk: Medium

---

**Status:** Draft

---

## Executive Summary

Implement the `_process_symbol()` method that handles the complete processing pipeline for a single symbol. This method is responsible for loading candle data from the repository, converting it to a pandas DataFrame, evaluating formulas against the data, and extracting signal matches. This is a core component of the ProcessingEngine that enables individual symbol processing within the broader data processing workflow.

---

## Execution Flow

1. Load candle data from the candle repository using symbol ID, timeframe, and date range
2. Handle the no-data case gracefully by returning an empty result
3. Convert the list of candles to a pandas DataFrame with OHLCV columns indexed by timestamp
4. Evaluate the provided formula against the DataFrame (if a formula is provided)
5. Extract signal matches from the evaluation results
6. Return a ProcessingResult containing matches, computed indicators, and processing time
7. Catch any exceptions and return an error result with appropriate logging

---

## User Stories

As a ProcessingEngine user, I want to process individual symbols so that I can:
- Load and analyze candle data for specific symbols and timeframes
- Evaluate trading formulas against historical price data
- Identify signal matches based on formula conditions
- Track processing time for performance monitoring

---

## Acceptance Scenarios

- Given a valid symbol, timeframe, and date range, when processing is requested, then candle data is loaded from the repository
- Given a successful data load, when conversion to DataFrame is performed, then OHLCV columns are created with timestamp as index
- Given a formula and data, when evaluation occurs, then signal matches are extracted for all matching candles
- Given no data available, when processing is requested, then an empty result is returned with appropriate error message
- Given an exception during processing, when it occurs, then it is caught, logged, and returned as an error result
- Given valid processing, when complete, then processing time is accurately recorded in milliseconds

---

## Requirements

### Functional Requirements

1. Implement `_process_symbol()` method with signature:
   - Parameters: symbol (str), timeframe (str), date_range (DateRange), formula (Optional[Formula]), parameters (dict)
   - Returns: ProcessingResult with matches, indicators, and processing time

2. Candle data loading:
   - Retrieve symbol ID from repository using symbol code
   - Query candle repository with symbol ID, timeframe, and date range
   - Handle missing data gracefully

3. DataFrame conversion:
   - Convert list of Candle objects to pandas DataFrame
   - Include OHLCV columns: timestamp, open, high, low, close, volume
   - Set timestamp as DataFrame index

4. Formula evaluation:
   - Evaluate formula against DataFrame if provided
   - Extract indicator values at matching timestamps
   - Return list of SignalMatch objects

5. Error handling:
   - Catch all exceptions during processing
   - Log errors appropriately
   - Return ProcessingResult with error message

6. Performance tracking:
   - Measure processing time using perf_counter
   - Return elapsed time in milliseconds

### Non-Functional Requirements

- Processing must complete efficiently for typical candle datasets
- Memory usage should be reasonable for DataFrames of varying sizes
- Error messages should be informative for debugging

---

## Key Entities

### ProcessingResult
- `symbol`: str - Symbol code
- `matches`: tuple[SignalMatch] - Identified signal matches
- `computed_indicators`: dict - Computed indicator values
- `processing_time_ms`: float - Processing duration
- `error`: Optional[str] - Error message if processing failed

### SignalMatch
- `timestamp`: datetime - Match timestamp
- `candle`: Candle - Associated candle data
- `signal_type`: SignalType - Type of signal (SCAN_HIT)
- `indicator_values`: dict[str, float] - Indicator values at match

### Supporting Classes
- `DateRange`: Date range specification (start, end)
- `Formula`: Formula definition for evaluation
- `Candle`: OHLCV candle data with timestamp

---

## Dependencies

- **E06-F02-T01-S01**: ProcessingEngine Skeleton - must be implemented first as foundation
- **E05**: DSL Parser & Evaluator (FormulaService) - required for formula evaluation
- **Candle Repository**: Data access layer for loading candle data
- **Symbol Repository**: Data access layer for symbol ID lookups
- **pandas**: DataFrame conversion and manipulation
- **FormulaService**: Formula evaluation service

---

## Gate Checks

- [ ] E06-F02-T01-S01 (ProcessingEngine Skeleton) is complete
- [ ] E05 (DSL Parser & Evaluator) is integrated and available
- [ ] Candle repository interface is defined and available
- [ ] Symbol repository interface is defined and available
- [ ] Core dependencies (pandas, datetime, logging) are available

---

## Tasks Preview

1. Implement `_process_symbol()` main method
2. Implement `_get_symbol_id()` helper method with caching consideration
3. Implement `_candles_to_dataframe()` DataFrame conversion method
4. Implement `_evaluate_formula()` formula evaluation and matching method
5. Implement `_extract_indicator_values()` indicator extraction method
6. Write unit tests for successful processing scenario
7. Write unit tests for no-data scenario
8. Write unit tests for error handling scenarios
9. Add performance benchmarking
10. Add integration testing with actual repositories

---

## Success Criteria

- [ ] `_process_symbol()` method fully implemented with complete functionality
- [ ] Candle data loading from repository working correctly
- [ ] DataFrame conversion with OHLCV columns functioning properly
- [ ] Formula evaluation with matches extraction operational
- [ ] Graceful handling of missing data scenarios
- [ ] Robust error handling with proper logging
- [ ] Accurate processing time tracking in milliseconds
- [ ] Comprehensive unit test coverage for all scenarios
- [ ] Code review approved
- [ ] Integration tests pass with actual data

---

## Risk Assessment

### Technical Risks

1. **Data Volume**: Large candle datasets may cause memory issues when converted to DataFrame
   - Mitigation: Implement chunking or streaming for large datasets; profile memory usage

2. **Symbol ID Caching**: Current implementation lacks caching, causing repeated lookups
   - Mitigation: Implement caching mechanism for symbol IDs in `_get_symbol_id()`

3. **Indicator Values Extraction**: Placeholder implementation needs enhancement
   - Mitigation: Integrate with computed indicators from formula evaluation pipeline

4. **Formula Service Integration**: Dependence on external FormulaService correctness
   - Mitigation: Implement defensive programming; validate formula results

### Operational Risks

1. **Missing Data Handling**: Current "No data available" error may mask underlying repository issues
   - Mitigation: Add detailed logging and error categorization

2. **Performance**: No current SLA or performance requirements specified
   - Mitigation: Establish and monitor performance baselines

---

## Notes and Clarifications

### Implementation Notes

- The `_get_symbol_id()` method includes a TODO for caching or symbol service integration
- The `_extract_indicator_values()` method has a TODO to integrate with computed indicators from the formula evaluation pipeline
- Processing time is measured using `time.perf_counter()` for high-resolution timing
- SignalMatch includes signal_type hardcoded as `SignalType.SCAN_HIT` (may need generalization)

### Questions for Clarification

1. Should symbol ID lookups be cached? If so, at what scope (session, instance, global)?
2. How should computed indicators be integrated with formula evaluation results?
3. Are there specific performance targets for processing latency?
4. Should the method support batch symbol processing with multi-threading?

### Future Enhancements

- Implement symbol ID caching layer
- Add support for streaming large datasets
- Enhance indicator value extraction to use formula-computed values
- Add performance profiling and optimization
- Implement batch processing optimization
- Add signal strength/confidence scoring

---

## Artifacts

### Code Artifacts

```python
class ProcessingEngine(IProcessingEngine):
    # ... existing code ...

    def _process_symbol(
        self,
        symbol: str,
        timeframe: str,
        date_range: DateRange,
        formula: Optional[Formula],
        parameters: dict[str, Any]
    ) -> ProcessingResult:
        """
        Process a single symbol.

        This method handles the complete processing pipeline for one symbol:
        1. Load candle data from repository
        2. Convert to pandas DataFrame
        3. Evaluate formula (if provided)
        4. Extract and return matches

        Args:
            symbol: Symbol code (e.g., "005930")
            timeframe: Candle timeframe (e.g., "1d", "1h")
            date_range: Date range for data
            formula: Formula to evaluate (optional)
            parameters: Additional parameters

        Returns:
            ProcessingResult with matches or error
        """
        start_time = time.perf_counter()

        try:
            # Load candle data
            symbol_id = self._get_symbol_id(symbol)
            candles = self._candle_repo.get_range(
                symbol_id=symbol_id,
                timeframe=timeframe,
                start=date_range.start,
                end=date_range.end
            )

            # Handle no data case
            if not candles:
                return ProcessingResult(
                    symbol=symbol,
                    matches=(),
                    computed_indicators={},
                    processing_time_ms=0,
                    error="No data available"
                )

            # Convert to DataFrame
            df = self._candles_to_dataframe(candles)

            # Evaluate formula if provided
            matches = []
            if formula:
                matches = self._evaluate_formula(df, candles, formula, parameters)

            elapsed = (time.perf_counter() - start_time) * 1000

            return ProcessingResult(
                symbol=symbol,
                matches=tuple(matches),
                computed_indicators={},
                processing_time_ms=elapsed
            )

        except Exception as e:
            logger.error(f"Error processing {symbol}: {e}")
            elapsed = (time.perf_counter() - start_time) * 1000
            return ProcessingResult(
                symbol=symbol,
                matches=(),
                computed_indicators={},
                processing_time_ms=elapsed,
                error=str(e)
            )

    def _get_symbol_id(self, symbol_code: str) -> int:
        """
        Get symbol ID from code.

        TODO: This should be cached or use a symbol service.
        """
        # For now, assume repository has a lookup method
        symbol = self._symbol_repo.get_by_code(symbol_code)
        if not symbol:
            raise ValueError(f"Unknown symbol: {symbol_code}")
        return symbol.id

    def _candles_to_dataframe(self, candles: list[Candle]) -> pd.DataFrame:
        """
        Convert candle list to pandas DataFrame.

        Creates a DataFrame with OHLCV columns indexed by timestamp.
        """
        df = pd.DataFrame([
            {
                'timestamp': c.timestamp,
                'open': c.open,
                'high': c.high,
                'low': c.low,
                'close': c.close,
                'volume': c.volume
            }
            for c in candles
        ])
        df.set_index('timestamp', inplace=True)
        return df

    def _evaluate_formula(
        self,
        df: pd.DataFrame,
        candles: list[Candle],
        formula: Formula,
        parameters: dict[str, Any]
    ) -> list[SignalMatch]:
        """
        Evaluate formula against DataFrame and extract matches.
        """
        result = self._formula_service.evaluate(formula, df, parameters)

        matches = []
        for idx, is_match in result.items():
            if is_match:
                candle_idx = df.index.get_loc(idx)
                candle = candles[candle_idx]
                matches.append(SignalMatch(
                    timestamp=idx,
                    candle=candle,
                    signal_type=SignalType.SCAN_HIT,
                    indicator_values=self._extract_indicator_values(df, idx)
                ))

        return matches

    def _extract_indicator_values(
        self,
        df: pd.DataFrame,
        timestamp: datetime
    ) -> dict[str, float]:
        """
        Extract indicator values at a specific timestamp.

        TODO: This should use computed indicators from pipeline.
        """
        return {
            'close': float(df.loc[timestamp, 'close']),
            'volume': float(df.loc[timestamp, 'volume'])
        }
```

### Test Artifacts

```python
def test_process_symbol_with_data():
    """Test successful symbol processing"""
    mock_repo = MagicMock()
    mock_repo.get_range.return_value = [
        Candle(timestamp=datetime(2024, 1, 1), open=100, high=105, low=99, close=104, volume=1000),
        Candle(timestamp=datetime(2024, 1, 2), open=104, high=108, low=103, close=107, volume=1200),
    ]

    mock_formula_service = MagicMock()
    mock_formula_service.evaluate.return_value = pd.Series(
        [False, True],
        index=[datetime(2024, 1, 1), datetime(2024, 1, 2)]
    )

    engine = ProcessingEngine(mock_repo, mock_formula_service)

    result = engine._process_symbol(
        symbol="005930",
        timeframe="1d",
        date_range=DateRange(date(2024, 1, 1), date(2024, 1, 31)),
        formula=mock_formula,
        parameters={}
    )

    assert result.error is None
    assert len(result.matches) == 1
    assert result.matches[0].timestamp == datetime(2024, 1, 2)

def test_process_symbol_no_data():
    """Test handling of missing data"""
    mock_repo = MagicMock()
    mock_repo.get_range.return_value = []

    engine = ProcessingEngine(mock_repo, MagicMock())

    result = engine._process_symbol(
        symbol="UNKNOWN",
        timeframe="1d",
        date_range=DateRange(date(2024, 1, 1), date(2024, 1, 31)),
        formula=None,
        parameters={}
    )

    assert result.error == "No data available"
    assert len(result.matches) == 0
```

---

## Metadata

| Field | Value |
|-------|-------|
| Spec ID | E06-F02-T01-S02 |
| Title | Single Symbol Processing |
| Type | Subtask |
| Parent | E06-F02-T01 |
| Epic | E06 |
| Feature | F02 |
| Task | T01 |
| Subtask | S02 |
| Status | Draft |
| Priority | Medium |
| Effort | M (Medium) |
| Risk | Medium |
| Domain | processing-engine |
| Created | - |
| Started | - |
| Completed | - |
| Deadline | - |
| Estimated Hours | - |
| Actual Hours | - |
