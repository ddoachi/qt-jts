# Spec: E06-F05 - Caching Layer

---

```yaml
id: E06-F05
clickup_task_id: null
title: Caching Layer
type: feature
parent: E06
children: []
epic: E06
feature: F05
domain: Processing Engine
status: draft
priority: medium
dates:
  start: null
  end: null
hours:
  estimated: null
  actual: null
tags:
  - caching
  - performance
  - concurrency
effort: medium
risk: low
```

---

**Status**: Draft

---

## Executive Summary

Implement caching mechanisms to avoid redundant computations and improve performance. Provides both indicator-level and result-level caching with LRU eviction and TTL expiration. This feature enables high-performance repeated scans by leveraging cached indicator values and processing results.

**Key Goals:**
- Achieve cache hit rate > 80% for repeated scans
- Enforce memory limits with LRU eviction (default 500MB)
- Implement TTL-based expiration for result cache (default 5 minutes)
- Ensure thread-safe concurrent access to caches

---

## Execution Flow

### Overview
1. **Initialization**: IndicatorCache and ProcessingResultCache are instantiated with configurable limits
2. **Cache Lookup**: System checks cache before computing indicators or processing results
3. **Cache Hit**: Return cached value if present and valid (not expired, within TTL)
4. **Cache Miss**: Compute value and store in cache with appropriate key/metadata
5. **Eviction**: When memory limits exceeded, LRU eviction removes least recently used entries
6. **Cleanup**: TTL-based expiration automatically removes stale entries from result cache

### Cache Key Generation
- **Indicator Cache Key**: `{symbol}:{timeframe}:{indicator}:{period}`
- **Result Cache Key**: Job ID (from ProcessingResult)

---

## User Stories

### US1: Indicator Value Caching
**As a** trading system analyst,
**I want to** have indicator values cached to avoid redundant calculations,
**So that** repeated analysis with the same parameters runs significantly faster.

**Acceptance Criteria:**
- Indicators are cached by symbol, timeframe, indicator type, and period
- Cache hits return values in < 10ms
- Memory usage stays within configured limits

### US2: Processing Result Caching
**As a** process engine component,
**I want to** cache full processing results for recent scans,
**So that** subsequent queries for the same job can retrieve results quickly.

**Acceptance Criteria:**
- Results are cached by job ID
- Cached results expire after TTL (default 5 minutes)
- Expired entries are automatically removed

### US3: Memory Management
**As a** system administrator,
**I want to** enforce memory limits on caches,
**So that** the application doesn't consume excessive memory.

**Acceptance Criteria:**
- Memory usage is tracked per cache
- LRU eviction removes entries when limit exceeded
- Memory limit is configurable (default 500MB)

### US4: Thread-Safe Operations
**As a** concurrent system,
**I want to** safely access caches from multiple threads,
**So that** race conditions and data corruption don't occur.

**Acceptance Criteria:**
- All cache operations are thread-safe
- No deadlocks under concurrent load
- Hit/miss tracking is accurate even under concurrent access

---

## Acceptance Scenarios

### Indicator Cache

**Scenario 1: LRU Eviction**
```
Given: IndicatorCache with max size 100MB
When: Adding indicators until memory limit exceeded
Then: Least recently used entry is evicted
  And: Memory stays within 100MB limit
  And: Most recently used entries are retained
```

**Scenario 2: Thread-Safe Operations**
```
Given: IndicatorCache with multiple reader/writer threads
When: Multiple threads access cache simultaneously
Then: All operations complete without race conditions
  And: Cache state remains consistent
  And: No deadlocks occur
```

**Scenario 3: Cache Key Generation**
```
Given: Indicator parameters (symbol=AAPL, timeframe=1H, indicator=SMA, period=20)
When: Cache key is generated
Then: Key is "{symbol}:{timeframe}:{indicator}:{period}"
  And: Key is deterministic and unique per parameter combination
```

**Scenario 4: Memory Size Estimation**
```
Given: pandas Series of indicator values
When: Memory size is estimated
Then: Estimation accounts for pandas overhead
  And: LRU eviction decisions are based on accurate sizes
```

### Result Cache

**Scenario 5: TTL-Based Expiration**
```
Given: ProcessingResultCache with TTL=300 seconds
When: Result is cached
Then: Result is accessible for 300 seconds
  And: After 300 seconds, result is considered expired
  And: Expired result is removed on next access or cleanup
```

**Scenario 6: Automatic Cleanup**
```
Given: ProcessingResultCache with expired entries
When: Cleanup is triggered
Then: All expired entries are removed
  And: Memory is freed
  And: Valid entries are retained
```

**Scenario 7: Cache Hit/Miss Tracking**
```
Given: Cache operations over time
When: Statistics are queried
Then: Hit rate, miss rate, and count are accurate
  And: Statistics reflect concurrent operations correctly
```

### Performance

**Scenario 8: High Hit Rate**
```
Given: Repeated scans with identical parameters
When: Cache is warm
Then: Cache hit rate exceeds 80%
  And: Response time is < 50ms per lookup
```

---

## Requirements

### Functional Requirements

1. **IndicatorCache Class**
   - Implement LRU eviction policy
   - Support configurable max size (default 500MB)
   - Generate cache keys from symbol, timeframe, indicator, period
   - Store pandas Series as values
   - Track memory usage accurately
   - Provide thread-safe operations with RLock

2. **ProcessingResultCache Class**
   - Store ProcessingResult objects indexed by job ID
   - Implement TTL-based expiration (default 300 seconds)
   - Provide automatic cleanup of expired entries
   - Track cache hit/miss statistics (optional)
   - Ensure thread-safe operations

3. **Cache Key Generation**
   - Deterministic key generation from parameters
   - Support hierarchical cache invalidation (if needed)
   - Validate key inputs

4. **Memory Management**
   - Estimate memory usage of cached pandas Series
   - Account for pandas internal overhead
   - Enforce maximum size limits
   - Trigger eviction before limit exceeded

5. **Thread Safety**
   - Use RLock for IndicatorCache
   - Use Lock for ProcessingResultCache
   - Prevent deadlocks in concurrent scenarios
   - Maintain data consistency

### Non-Functional Requirements

1. **Performance**
   - Cache lookup < 10ms
   - LRU eviction overhead < 5% of system time
   - TTL cleanup < 1ms per expired entry

2. **Memory Efficiency**
   - Memory usage within configured limits
   - No memory leaks
   - Efficient eviction algorithm

3. **Testability**
   - > 85% code coverage
   - Unit tests for all public methods
   - Integration tests with Processing Engine

4. **Observability**
   - Log cache operations (optional: at debug level)
   - Track hit rate and miss rate
   - Monitor memory usage

---

## Key Entities

### IndicatorCache
- **Purpose**: Cache indicator calculations by symbol, timeframe, indicator type, and period
- **Key Structure**: `{symbol}:{timeframe}:{indicator}:{period}`
- **Value Type**: `pandas.Series`
- **Eviction Policy**: LRU (Least Recently Used)
- **Max Size**: Configurable, default 500MB
- **Thread Safety**: RLock

### ProcessingResultCache
- **Purpose**: Cache full processing results by job ID
- **Key Structure**: Job ID (string)
- **Value Type**: `ProcessingResult`
- **Eviction Policy**: TTL (Time To Live)
- **Default TTL**: 300 seconds (5 minutes)
- **Thread Safety**: Lock

### CacheKey
- **Purpose**: Utility class for cache key generation
- **Methods**:
  - `generate_indicator_key(symbol, timeframe, indicator, period) -> str`
  - `validate_key(key) -> bool`

### MemoryTracker
- **Purpose**: Track and estimate memory usage of cached objects
- **Methods**:
  - `estimate_series_size(series) -> int` (bytes)
  - `get_cache_size() -> int`
  - `check_memory_limit(size, limit) -> bool`

---

## Dependencies

### Feature Dependencies
- **E06-F02**: Processing Engine (provides ProcessingResult class, consumes result caching)
- **E06-F03**: Indicator Pipeline (provides indicator computation, feeds into indicator caching)

### External Dependencies
- **pandas**: For Series memory estimation
- **threading**: For RLock and Lock primitives
- **functools**: For LRU utilities (if used)
- **time**: For TTL calculations

### Internal Dependencies
- ProcessingResult class from E06-F02
- Indicator computation pipeline from E06-F03

---

## Gate Checks

- [ ] PRD requirements aligned with E06 specifications
- [ ] Caching design reviewed and approved
- [ ] Dependencies (E06-F02, E06-F03) confirmed available
- [ ] Thread safety approach validated
- [ ] Memory estimation approach confirmed
- [ ] Performance targets (>80% hit rate) feasible
- [ ] Test strategy defined and approved
- [ ] No conflicts with existing caching approaches

---

## Tasks Preview

| Task ID | Title | Effort | Dependencies | Status |
|---------|-------|--------|--------------|--------|
| E06-F05-T01 | Implement IndicatorCache with LRU eviction | M | F03 | pending |
| E06-F05-T02 | Implement ProcessingResultCache with TTL | M | F02 | pending |
| E06-F05-T03 | Implement cache utilities (key generation, memory tracking) | S | T01, T02 | pending |
| E06-F05-T04 | Write unit tests for IndicatorCache | M | T01 | pending |
| E06-F05-T05 | Write unit tests for ProcessingResultCache | M | T02 | pending |
| E06-F05-T06 | Write integration tests with Processing Engine | L | T02, F02 | pending |

---

## Success Criteria

1. **Functional Correctness**
   - [ ] IndicatorCache correctly implements LRU eviction
   - [ ] ProcessingResultCache correctly implements TTL expiration
   - [ ] Cache keys are generated consistently
   - [ ] Thread-safe operations validated under load

2. **Performance Targets**
   - [ ] Cache hit rate > 80% for repeated scans
   - [ ] Cache lookup time < 10ms
   - [ ] Memory limits enforced (default 500MB)
   - [ ] No performance degradation under high concurrency

3. **Code Quality**
   - [ ] Test coverage > 85%
   - [ ] All acceptance criteria met
   - [ ] Code review approved
   - [ ] No critical issues found in static analysis

4. **Integration**
   - [ ] Seamlessly integrates with Processing Engine (E06-F02)
   - [ ] Seamlessly integrates with Indicator Pipeline (E06-F03)
   - [ ] No breaking changes to existing APIs
   - [ ] Backward compatible

---

## Risk Assessment

### Risk 1: Memory Leak in LRU Eviction
**Probability**: Medium | **Impact**: High
- **Description**: Incorrect eviction logic could leak memory instead of freeing it
- **Mitigation**: Implement careful eviction logic with thorough testing; use weak references where appropriate
- **Contingency**: Implement manual cache clearing function

### Risk 2: Thread Safety Issues
**Probability**: Medium | **Impact**: High
- **Description**: Race conditions or deadlocks under concurrent load
- **Mitigation**: Use RLock/Lock primitives; design for simplicity; test with concurrent scenarios
- **Contingency**: Implement fallback sequential cache access

### Risk 3: Inaccurate Memory Estimation
**Probability**: Low | **Impact**: Medium
- **Description**: Memory limit enforcement fails due to incorrect size calculation
- **Mitigation**: Implement conservative estimation; account for pandas overhead; validate with benchmarks
- **Contingency**: Use system memory sampling as fallback

### Risk 4: Cache Hit Rate Below Target
**Probability**: Low | **Impact**: Low
- **Description**: Cache hit rate doesn't reach >80% target
- **Mitigation**: Profile real usage patterns; adjust TTL and cache sizes; optimize key generation
- **Contingency**: Accept lower hit rate; focus on memory efficiency

### Risk 5: TTL Cleanup Performance
**Probability**: Low | **Impact**: Medium
- **Description**: TTL cleanup operation is slow and blocks cache access
- **Mitigation**: Implement background cleanup; use efficient expiration checking; limit cleanup scope
- **Contingency**: Implement lazy expiration (check on access only)

---

## Notes and Clarifications

### Cache Configuration
- **IndicatorCache Size**: Default 500MB, configurable via constructor
- **ResultCache TTL**: Default 300 seconds (5 minutes), configurable via constructor
- **Memory Estimation**: Should account for pandas Series overhead (index, dtype, etc.)

### Threading Model
- **IndicatorCache**: Uses RLock to allow re-entrant read access
- **ResultCache**: Uses Lock for simpler single-threaded-per-operation semantics
- **Hit/Miss Tracking**: Must be atomic or protected to ensure accuracy under concurrency

### Eviction Strategy
- **IndicatorCache**: LRU eviction removes least recently used entry when limit exceeded
- **ResultCache**: Lazy TTL expiration removes expired entries on access or periodic cleanup
- **Hybrid Approach**: Could implement age-based + size-based eviction for result cache

### Integration Points
1. **Processing Engine (E06-F02)**
   - Calls result cache before and after processing
   - Provides ProcessingResult objects to cache

2. **Indicator Pipeline (E06-F03)**
   - Calls indicator cache before computing indicators
   - Provides computed Series to cache

### Performance Considerations
- Cache lookup should have minimal overhead (< 1ms expected)
- Eviction operations should be amortized O(1) for LRU
- TTL checking should be O(1) per entry
- Memory estimation should be fast (pre-computed or cached)

### Optional Enhancements
- Cache hit/miss statistics and reporting
- Cache warming strategies
- Adaptive TTL based on access patterns
- Distributed caching (future version)
- Persistent cache (future version)

---

## Artifacts

### Code Artifacts
- `cache/indicator_cache.py` - IndicatorCache implementation
- `cache/result_cache.py` - ProcessingResultCache implementation
- `cache/cache_key.py` - Cache key generation utilities
- `cache/memory_tracker.py` - Memory tracking and estimation
- `tests/test_indicator_cache.py` - IndicatorCache unit tests
- `tests/test_result_cache.py` - ProcessingResultCache unit tests
- `tests/test_cache_integration.py` - Integration tests with Processing Engine

### Documentation Artifacts
- Cache design documentation
- API documentation for IndicatorCache and ProcessingResultCache
- Performance benchmark results
- Thread safety analysis documentation

### Test Artifacts
- Unit test suite (> 85% coverage target)
- Integration test suite
- Performance test suite
- Concurrency test suite (stress testing with multiple threads)

---

## Metadata

**Created**: 2025-12-28
**Last Updated**: 2025-12-28
**Version**: 1.0
**Author**: System
**Related Specs**:
  - E06.spec.md (Section 5: Caching Layer)
  - E06-F02.spec.md (Processing Engine)
  - E06-F03.spec.md (Indicator Pipeline)
**PRD References**: Section 5.2, 5.5
**Epic**: E06 (Processing Engine)
