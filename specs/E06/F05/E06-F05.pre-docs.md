# E06-F05: Caching Layer - Pre-Implementation Planning

## 1. Overview and Objectives

### 1.1 Purpose
Implement a dual-layer caching system to optimize the processing engine's performance by eliminating redundant computations. The caching layer consists of:
- **IndicatorCache**: LRU-based cache for technical indicator calculations
- **ProcessingResultCache**: TTL-based cache for complete processing results

### 1.2 Key Objectives
1. Achieve **>80% cache hit rate** for repeated scans across the same symbols
2. Enforce **memory limits** with automatic LRU eviction (default 500MB for indicators)
3. Provide **TTL-based expiration** for result caching (default 5 minutes)
4. Ensure **thread-safe** operations for concurrent access patterns
5. Minimize **cache operation overhead** (<5ms per lookup/insert)

### 1.3 Success Metrics
- Cache hit rate >80% for repeated symbol scans
- Memory usage stays within configured limits
- Cache lookup time <5ms (p95)
- Thread-safe under concurrent load (verified via tests)
- Test coverage >85%

---

## 2. Technical Approach

### 2.1 Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                      Caching Layer                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌────────────────────────┐      ┌──────────────────────────┐  │
│  │   IndicatorCache       │      │  ProcessingResultCache   │  │
│  │   (LRU Eviction)       │      │  (TTL Expiration)        │  │
│  ├────────────────────────┤      ├──────────────────────────┤  │
│  │ - Key: Symbol+Params   │      │ - Key: Job ID            │  │
│  │ - Value: pd.Series     │      │ - Value: ProcessingResult│  │
│  │ - Max Size: 500MB      │      │ - TTL: 300s              │  │
│  │ - Thread-safe: RLock   │      │ - Thread-safe: RLock     │  │
│  └────────────────────────┘      └──────────────────────────┘  │
│           ▲                                    ▲                │
│           │                                    │                │
└───────────┼────────────────────────────────────┼────────────────┘
            │                                    │
    ┌───────┴────────┐                  ┌────────┴────────┐
    │ Indicator      │                  │ Processing      │
    │ Pipeline       │                  │ Engine          │
    │ (F03)          │                  │ (F02)           │
    └────────────────┘                  └─────────────────┘
```

### 2.2 Cache Key Design

#### Indicator Cache Key Format
```python
"{symbol}:{timeframe}:{indicator}:{period}"
Examples:
- "AAPL:1D:SMA:20"
- "005930:1H:RSI:14"
- "TSLA:5M:EMA:12"
```

**Rationale**: Composite key ensures unique identification of each indicator calculation while enabling efficient lookups.

#### Result Cache Key Format
```python
"{job_id}"
Examples:
- "scan_20250101_123456"
- "backtest_job_789"
```

**Rationale**: Job IDs are already unique and directly map to processing results.

### 2.3 Memory Management Strategy

#### IndicatorCache Memory Tracking
1. **Size Estimation**: Calculate memory size of pandas Series using `sys.getsizeof()` + underlying data buffer
2. **LRU Eviction**: When inserting would exceed limit, evict least-recently-used entries until space available
3. **Ordered Tracking**: Use `OrderedDict` to maintain insertion/access order for O(1) LRU operations

#### ProcessingResultCache TTL Management
1. **Timestamp Tracking**: Store insertion timestamp with each entry
2. **Lazy Expiration**: Check TTL on access; return `None` if expired
3. **Periodic Cleanup**: Provide `clear_expired()` method for background cleanup

### 2.4 Thread Safety Approach

Both caches will use **`threading.RLock()`** (reentrant lock) to ensure:
- Atomic read-modify-write operations
- Protection against race conditions during eviction
- Deadlock prevention when cache operations call each other

**Lock Granularity**: Per-cache instance (coarse-grained) for simplicity and correctness over maximum throughput.

---

## 3. Dependencies and Prerequisites

### 3.1 Required Dependencies

| Dependency | Version | Purpose |
|------------|---------|---------|
| `pandas` | >=1.5.0 | Storing indicator Series |
| `threading` | stdlib | Thread synchronization |
| `collections` | stdlib | OrderedDict for LRU |
| `sys` | stdlib | Memory size estimation |
| `time` | stdlib | TTL timestamp tracking |

### 3.2 Feature Dependencies

- **E06-F03 (Indicator Pipeline)**: Must be implemented first; provides indicators to cache
- **E06-F02 (Processing Engine)**: Must be implemented first; provides results to cache
- **E06-F01 (Domain Model)**: Defines `ProcessingResult`, `SignalMatch` data structures

### 3.3 Integration Points

1. **IndicatorPipeline** (F03) should check cache before computing
2. **ProcessingEngine** (F02) should check result cache before processing
3. Both should insert computed values into respective caches

---

## 4. Implementation Plan

### 4.1 Phase 1: IndicatorCache (Task E06-F05-T01)

**Estimated Effort**: Medium (2-3 days)

#### Step 1.1: Core Cache Implementation
- Create `IndicatorCache` class with `OrderedDict` backing store
- Implement `get(key)` and `put(key, value)` methods
- Add `RLock` for thread safety

#### Step 1.2: Memory Management
- Implement `_estimate_size(series)` for pandas Series
- Track total memory usage in `self._current_size`
- Implement `_evict_lru()` to remove oldest entries until under limit

#### Step 1.3: Key Generation Utilities
- Create `generate_indicator_key(symbol, timeframe, indicator, period)` utility
- Add validation for key components

#### Step 1.4: Observability (Optional)
- Add cache hit/miss counters
- Implement `get_stats()` method for monitoring

### 4.2 Phase 2: ProcessingResultCache (Task E06-F05-T02)

**Estimated Effort**: Medium (2-3 days)

#### Step 2.1: Core Cache Implementation
- Create `ProcessingResultCache` class with `dict` backing store
- Store tuples of `(result, timestamp)` as values
- Implement `get(job_id)` with TTL check
- Implement `put(job_id, result)` with timestamp

#### Step 2.2: TTL Management
- Implement `_is_expired(timestamp)` check
- Create `clear_expired()` method to remove all expired entries
- Add configurable TTL parameter (default 300s)

#### Step 2.3: Thread Safety
- Add `RLock` protection for all cache operations
- Ensure atomic TTL checks and removals

### 4.3 Phase 3: Testing

**Estimated Effort**: Medium (2 days)

#### Step 3.1: Unit Tests
- Test basic cache operations (get, put, contains)
- Test LRU eviction under memory pressure
- Test TTL expiration behavior
- Test concurrent access (multi-threading)

#### Step 3.2: Integration Tests
- Test integration with IndicatorPipeline (F03)
- Test integration with ProcessingEngine (F02)
- Verify cache hit rates in realistic scenarios

#### Step 3.3: Performance Tests
- Benchmark cache lookup time
- Measure memory overhead
- Verify eviction performance under load

---

## 5. File Structure and Locations

### 5.1 Source Files

Assuming project structure follows Python best practices:

```
project_root/
├── src/
│   └── processing/
│       ├── __init__.py
│       ├── cache/
│       │   ├── __init__.py
│       │   ├── indicator_cache.py      # IndicatorCache class
│       │   ├── result_cache.py         # ProcessingResultCache class
│       │   └── cache_keys.py           # Key generation utilities
│       ├── engine/
│       │   └── processing_engine.py    # Integration point
│       └── indicators/
│           └── indicator_pipeline.py   # Integration point
```

### 5.2 Test Files

```
tests/
├── unit/
│   └── processing/
│       ├── test_indicator_cache.py     # IndicatorCache tests
│       ├── test_result_cache.py        # ResultCache tests
│       └── test_cache_keys.py          # Key utility tests
└── integration/
    └── processing/
        └── test_cache_integration.py   # Integration with F02/F03
```

### 5.3 Configuration Files

Cache configuration should be added to application config:

```yaml
# config/processing.yaml
cache:
  indicator:
    max_size_mb: 500
    enabled: true
  result:
    ttl_seconds: 300
    enabled: true
    cleanup_interval_seconds: 60
```

---

## 6. Key Interfaces and Contracts

### 6.1 IndicatorCache Interface

```python
class IndicatorCache:
    """LRU cache for technical indicator values"""

    def __init__(self, max_size_bytes: int = 500 * 1024 * 1024):
        """Initialize cache with memory limit (default 500MB)"""

    def get(self, key: str) -> Optional[pd.Series]:
        """Retrieve cached indicator, returns None if not found"""

    def put(self, key: str, value: pd.Series) -> None:
        """Store indicator in cache, evicting LRU if needed"""

    def contains(self, key: str) -> bool:
        """Check if key exists in cache"""

    def clear(self) -> None:
        """Clear all cached entries"""

    def get_stats(self) -> Dict[str, Any]:
        """Return cache statistics (hits, misses, size, count)"""
```

### 6.2 ProcessingResultCache Interface

```python
class ProcessingResultCache:
    """TTL-based cache for processing results"""

    def __init__(self, ttl_seconds: int = 300):
        """Initialize cache with TTL (default 300s)"""

    def get(self, job_id: str) -> Optional[ProcessingResult]:
        """Retrieve cached result, returns None if expired/not found"""

    def put(self, job_id: str, result: ProcessingResult) -> None:
        """Store result in cache with current timestamp"""

    def contains(self, job_id: str) -> bool:
        """Check if job_id exists and is not expired"""

    def clear_expired(self) -> int:
        """Remove all expired entries, return count removed"""

    def clear(self) -> None:
        """Clear all cached entries"""
```

### 6.3 Cache Key Utilities

```python
def generate_indicator_key(
    symbol: str,
    timeframe: str,
    indicator: str,
    period: Optional[int] = None,
    **kwargs
) -> str:
    """Generate cache key for indicator lookup"""

def parse_indicator_key(key: str) -> Dict[str, str]:
    """Parse cache key back into components"""
```

### 6.4 Integration Contracts

**IndicatorPipeline Integration**:
```python
# In IndicatorPipeline.compute_indicator()
key = generate_indicator_key(symbol, timeframe, indicator, period)
cached = self.cache.get(key)
if cached is not None:
    return cached

result = self._compute(...)
self.cache.put(key, result)
return result
```

**ProcessingEngine Integration**:
```python
# In ProcessingEngine.process_batch()
cached_result = self.result_cache.get(job.id)
if cached_result is not None:
    return cached_result

result = self._process(...)
self.result_cache.put(job.id, result)
return result
```

---

## 7. Testing Strategy

### 7.1 Unit Test Coverage

#### IndicatorCache Tests
- `test_basic_get_put` - Verify basic cache operations
- `test_lru_eviction` - Fill cache beyond limit, verify LRU eviction
- `test_memory_tracking` - Verify accurate memory size estimation
- `test_key_generation` - Test key utility functions
- `test_concurrent_access` - Multi-threaded stress test
- `test_cache_stats` - Verify hit/miss tracking

#### ResultCache Tests
- `test_basic_get_put` - Verify basic cache operations
- `test_ttl_expiration` - Insert entry, wait TTL, verify expired
- `test_clear_expired` - Mix of expired/valid entries, verify cleanup
- `test_concurrent_access` - Multi-threaded stress test
- `test_ttl_update` - Verify updating existing entry resets TTL

### 7.2 Integration Test Coverage

- `test_indicator_pipeline_caching` - Verify F03 uses cache correctly
- `test_processing_engine_caching` - Verify F02 uses cache correctly
- `test_cache_hit_rate` - Run repeated scans, measure hit rate >80%
- `test_memory_limits_enforced` - Process large dataset, verify no OOM

### 7.3 Performance Test Coverage

- `benchmark_cache_lookup` - Measure p50/p95/p99 lookup times
- `benchmark_eviction` - Measure eviction overhead under pressure
- `benchmark_concurrent_access` - Measure throughput with N threads
- `test_memory_overhead` - Compare cached vs uncached memory usage

### 7.4 Test Data

Use realistic test data:
- **Small dataset**: 10 symbols, 1000 candles each
- **Medium dataset**: 100 symbols, 5000 candles each
- **Large dataset**: 1000 symbols, 10000 candles each

---

## 8. Risks and Mitigations

### 8.1 Risk: Memory Size Estimation Inaccuracy

**Description**: `sys.getsizeof()` may not accurately reflect true memory usage of pandas Series, leading to OOM or excessive eviction.

**Impact**: High - Could cause application crashes or poor cache efficiency

**Mitigation**:
1. Use `series.memory_usage(deep=True)` instead of `getsizeof()`
2. Add safety margin (10%) to max cache size
3. Implement monitoring alerts for memory usage
4. Add fallback to clear entire cache if memory threshold exceeded

**Status**: Mitigated by implementation approach

### 8.2 Risk: Lock Contention Under High Concurrency

**Description**: Coarse-grained locking may create bottleneck with many concurrent cache accesses.

**Impact**: Medium - Could limit scalability to high core counts

**Mitigation**:
1. Start with coarse-grained locking for correctness
2. Benchmark under realistic load (4-8 cores)
3. If bottleneck confirmed, refactor to sharded cache with per-shard locks
4. Consider lock-free data structures (e.g., `lru_cache` from `functools`)

**Status**: Accept for initial implementation, optimize if needed

### 8.3 Risk: Cache Invalidation Complexity

**Description**: No mechanism to invalidate cache when underlying data changes (e.g., new candles added).

**Impact**: Low - Mitigated by TTL for results; indicators are immutable for historical data

**Mitigation**:
1. Result cache has TTL, so stale data expires automatically
2. Indicator cache is keyed by specific parameters, so changes create new keys
3. Document cache behavior and limitations
4. Consider adding manual invalidation API for future needs

**Status**: Accept for v1, defer invalidation API to future iteration

### 8.4 Risk: TTL Cleanup Overhead

**Description**: If many entries expire simultaneously, `clear_expired()` could be slow and block cache access.

**Impact**: Low - Cleanup is infrequent and cache lock is held briefly

**Mitigation**:
1. Make `clear_expired()` optional/background task
2. Lazy expiration on access reduces cleanup pressure
3. Consider batching cleanup or async execution
4. Monitor cleanup execution time

**Status**: Low risk, monitor in production

### 8.5 Risk: Pandas Series Serialization for Distributed Cache

**Description**: Future requirement for distributed caching (Redis) will require serialization, which is out of scope.

**Impact**: Low - Explicitly deferred to future work

**Mitigation**:
1. Design cache interface to be backend-agnostic
2. Document serialization requirements for future work
3. Consider using pickle-compatible data structures

**Status**: Deferred, not blocking v1

### 8.6 Risk: Integration Testing Dependencies

**Description**: Cannot fully test caching without F02 and F03 implementations.

**Impact**: Medium - May delay testing phase

**Mitigation**:
1. Implement caches with mock/stub dependencies
2. Create integration test placeholders to run once F02/F03 complete
3. Focus on unit tests for cache logic in isolation

**Status**: Plan for sequential testing approach

---

## 9. Performance Considerations

### 9.1 Expected Cache Hit Rates

Based on typical usage patterns:
- **Repeated scans**: 80-90% hit rate (same symbols, timeframes)
- **Backtesting**: 60-70% hit rate (overlapping time windows)
- **Real-time monitoring**: 40-50% hit rate (new data arrives frequently)

### 9.2 Memory Footprint

Estimated memory usage per cached indicator:
- 1000 candles: ~8KB (float64 array)
- 10,000 candles: ~80KB
- 100,000 candles: ~800KB

With 500MB limit:
- Can cache ~600 indicators with 100K candles each
- Or ~6000 indicators with 10K candles each

### 9.3 Lookup Performance

Target metrics:
- Cache lookup: <5ms (p95)
- Cache insertion: <10ms (p95)
- LRU eviction: <50ms (p95) when triggered

### 9.4 Concurrency Scalability

Expected to scale linearly up to 8 cores with coarse-grained locking. Beyond that, may need sharding.

---

## 10. Acceptance Checklist

Before considering E06-F05 complete:

- [ ] IndicatorCache implemented with LRU eviction
- [ ] ProcessingResultCache implemented with TTL
- [ ] Cache key generation utilities implemented
- [ ] Thread safety verified with concurrent tests
- [ ] Memory limits enforced and tested
- [ ] Integration with IndicatorPipeline (F03) complete
- [ ] Integration with ProcessingEngine (F02) complete
- [ ] Unit test coverage >85%
- [ ] Performance benchmarks meet targets
- [ ] Documentation updated (API docs, usage examples)
- [ ] Code review completed
- [ ] All acceptance criteria from spec met

---

## 11. References

- **Feature Spec**: `/mnt/c/Users/ddoachi/dev/project-qt-jts/worktrees/Split-E06/specs/E06/F05/E06-F05.spec.md`
- **Epic Spec**: `/mnt/c/Users/ddoachi/dev/project-qt-jts/worktrees/Split-E06/specs/E06/E06.spec.md`
- **Dependency F02**: `/mnt/c/Users/ddoachi/dev/project-qt-jts/worktrees/Split-E06/specs/E06/F02/E06-F02.spec.md`
- **Dependency F03**: `/mnt/c/Users/ddoachi/dev/project-qt-jts/worktrees/Split-E06/specs/E06/F03/E06-F03.spec.md`

---

*Document generated: 2025-12-28*
*Target implementation start: After E06-F02 and E06-F03 completion*
