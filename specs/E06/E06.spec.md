# Epic E06: Processing Engine

## Metadata

| Field | Value |
|-------|-------|
| Epic ID | E06 |
| Title | Processing Engine |
| Status | Draft |
| Platform | Cross-platform |
| Dependencies | E02 (Storage), E05 (DSL) |
| PRD Sections | 5.2, 5.3, 5.5 (underlying engine) |

---

## 1. Overview

### 1.1 Purpose

Provide a high-performance computation engine for:
- Batch processing of formulas across symbols
- Technical indicator calculation pipeline
- Signal generation and event detection
- Parallel execution for multi-symbol analysis

### 1.2 Goals

1. **High Performance**: Process 2,500 symbols in < 30 seconds
2. **Scalable**: Utilize multiple CPU cores
3. **Memory Efficient**: Stream processing for large datasets
4. **Reusable**: Core engine used by Scanner, Backtesting, and Trading

---

## 2. Architecture

### 2.1 Pipeline Design

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        Processing Pipeline                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌──────────┐ │
│  │  Data       │───►│  Indicator  │───►│  Formula    │───►│  Signal  │ │
│  │  Loader     │    │  Calculator │    │  Evaluator  │    │  Emitter │ │
│  │             │    │             │    │             │    │          │ │
│  │  Load OHLCV │    │  Calc RSI,  │    │  Eval DSL   │    │  Emit    │ │
│  │  from DB    │    │  SMA, etc.  │    │  expression │    │  signals │ │
│  └─────────────┘    └─────────────┘    └─────────────┘    └──────────┘ │
│        │                   │                   │                  │     │
│        └───────────────────┴───────────────────┴──────────────────┘     │
│                                    │                                     │
│                              ┌─────┴─────┐                              │
│                              │  Results  │                              │
│                              │  Collector│                              │
│                              └───────────┘                              │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.2 Multi-Symbol Processing

```
┌─────────────────────────────────────────────────────────────────────────┐
│                      Parallel Symbol Processing                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Symbol Queue                    Worker Pool (N cores)                   │
│  ┌─────────────┐                ┌───────────────────┐                   │
│  │ 005930      │───┬───────────►│    Worker 1       │──┐                │
│  │ 000660      │   │            │    (Process)      │  │                │
│  │ 035420      │   │            └───────────────────┘  │                │
│  │ 051910      │   │            ┌───────────────────┐  │                │
│  │ 006400      │───┼───────────►│    Worker 2       │──┼──► Results    │
│  │ ...         │   │            │    (Process)      │  │                │
│  │             │   │            └───────────────────┘  │                │
│  │             │   │            ┌───────────────────┐  │                │
│  │             │───┴───────────►│    Worker N       │──┘                │
│  │             │                │    (Process)      │                   │
│  └─────────────┘                └───────────────────┘                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 3. Domain Model

### 3.1 Processing Job

```python
@dataclass
class ProcessingJob:
    """A batch processing job"""
    id: str
    job_type: JobType  # SCAN, BACKTEST, PATTERN_DISCOVERY
    symbols: list[str]
    timeframe: str
    date_range: DateRange
    formula: Optional[Formula]
    parameters: dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.utcnow)

@dataclass
class ProcessingResult:
    """Result from processing a single symbol"""
    symbol: str
    matches: list[SignalMatch]
    computed_indicators: dict[str, pd.Series]
    processing_time_ms: float
    error: Optional[str] = None

@dataclass
class SignalMatch:
    """A formula match at a specific point"""
    timestamp: datetime
    candle: Candle
    signal_type: SignalType  # ENTRY, EXIT, SCAN_HIT
    indicator_values: dict[str, float]
    metadata: dict[str, Any] = field(default_factory=dict)
```

### 3.2 Processing Context

```python
@dataclass
class ProcessingContext:
    """Context for processing a symbol"""
    symbol: Symbol
    candles: CandleSeries
    indicators: dict[str, pd.Series] = field(default_factory=dict)

    def add_indicator(self, name: str, values: pd.Series) -> None:
        self.indicators[name] = values

    def get_indicator(self, name: str) -> Optional[pd.Series]:
        return self.indicators.get(name)
```

---

## 4. Processing Engine

### 4.1 Core Engine

```python
class ProcessingEngine:
    """Core processing engine for formula evaluation"""

    def __init__(
        self,
        candle_repo: ICandleRepository,
        formula_service: FormulaService,
        max_workers: int = None
    ):
        self._candle_repo = candle_repo
        self._formula_service = formula_service
        self._max_workers = max_workers or (os.cpu_count() or 4)
        self._executor = ProcessPoolExecutor(max_workers=self._max_workers)

    async def process_batch(
        self,
        job: ProcessingJob,
        progress_callback: Callable[[int, int], None] = None
    ) -> list[ProcessingResult]:
        """Process multiple symbols in parallel"""
        results = []
        total = len(job.symbols)

        # Create futures for parallel execution
        futures = []
        for symbol in job.symbols:
            future = self._executor.submit(
                self._process_symbol,
                symbol,
                job.timeframe,
                job.date_range,
                job.formula,
                job.parameters
            )
            futures.append((symbol, future))

        # Collect results as they complete
        completed = 0
        for symbol, future in futures:
            try:
                result = future.result(timeout=60)
                results.append(result)
            except Exception as e:
                results.append(ProcessingResult(
                    symbol=symbol,
                    matches=[],
                    computed_indicators={},
                    processing_time_ms=0,
                    error=str(e)
                ))

            completed += 1
            if progress_callback:
                progress_callback(completed, total)

        return results

    def _process_symbol(
        self,
        symbol: str,
        timeframe: str,
        date_range: DateRange,
        formula: Optional[Formula],
        parameters: dict[str, Any]
    ) -> ProcessingResult:
        """Process a single symbol (runs in worker process)"""
        start_time = time.perf_counter()

        # Load candle data
        candles = self._candle_repo.get_range(
            symbol_id=self._get_symbol_id(symbol),
            timeframe=timeframe,
            start=date_range.start,
            end=date_range.end
        )

        if not candles:
            return ProcessingResult(
                symbol=symbol,
                matches=[],
                computed_indicators={},
                processing_time_ms=0,
                error="No data available"
            )

        # Create DataFrame
        df = pd.DataFrame([asdict(c) for c in candles])
        df.set_index('timestamp', inplace=True)

        # Evaluate formula if provided
        matches = []
        if formula:
            result = self._formula_service.evaluate(formula, df, parameters)

            # Extract matches
            for idx, is_match in result.items():
                if is_match:
                    candle = candles[df.index.get_loc(idx)]
                    matches.append(SignalMatch(
                        timestamp=idx,
                        candle=candle,
                        signal_type=SignalType.SCAN_HIT,
                        indicator_values=self._extract_indicator_values(df, idx)
                    ))

        elapsed = (time.perf_counter() - start_time) * 1000

        return ProcessingResult(
            symbol=symbol,
            matches=matches,
            computed_indicators={},  # Optionally include computed series
            processing_time_ms=elapsed
        )
```

### 4.2 Indicator Pipeline

```python
class IndicatorPipeline:
    """Pre-compute common indicators for efficiency"""

    def __init__(self):
        self._indicators: dict[str, Callable] = {}
        self._register_defaults()

    def _register_defaults(self):
        """Register common indicators"""
        self.register('sma_20', lambda df: df['close'].rolling(20).mean())
        self.register('sma_50', lambda df: df['close'].rolling(50).mean())
        self.register('sma_200', lambda df: df['close'].rolling(200).mean())
        self.register('ema_12', lambda df: df['close'].ewm(span=12).mean())
        self.register('ema_26', lambda df: df['close'].ewm(span=26).mean())
        self.register('rsi_14', self._calc_rsi_14)
        self.register('atr_14', self._calc_atr_14)
        self.register('volume_sma_20', lambda df: df['volume'].rolling(20).mean())

    def register(self, name: str, calculator: Callable[[pd.DataFrame], pd.Series]):
        self._indicators[name] = calculator

    def compute(self, df: pd.DataFrame, indicators: list[str] = None) -> dict[str, pd.Series]:
        """Compute requested indicators"""
        result = {}
        to_compute = indicators or list(self._indicators.keys())

        for name in to_compute:
            if name in self._indicators:
                result[name] = self._indicators[name](df)

        return result

    def _calc_rsi_14(self, df: pd.DataFrame) -> pd.Series:
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0)
        loss = (-delta).where(delta < 0, 0)
        avg_gain = gain.rolling(14).mean()
        avg_loss = loss.rolling(14).mean()
        rs = avg_gain / avg_loss
        return 100 - (100 / (1 + rs))

    def _calc_atr_14(self, df: pd.DataFrame) -> pd.Series:
        high, low, close = df['high'], df['low'], df['close']
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        return tr.rolling(14).mean()
```

### 4.3 Signal Aggregator

```python
class SignalAggregator:
    """Aggregate and analyze signals across symbols"""

    def aggregate(
        self,
        results: list[ProcessingResult],
        sort_by: str = 'timestamp'
    ) -> AggregatedSignals:
        """Combine results from multiple symbols"""
        all_matches = []

        for result in results:
            if result.error:
                continue
            for match in result.matches:
                all_matches.append({
                    'symbol': result.symbol,
                    'timestamp': match.timestamp,
                    'candle': match.candle,
                    'signal_type': match.signal_type,
                    'indicators': match.indicator_values
                })

        # Sort by requested field
        all_matches.sort(key=lambda m: m[sort_by])

        return AggregatedSignals(
            total_symbols=len(results),
            symbols_with_matches=len([r for r in results if r.matches]),
            total_matches=len(all_matches),
            matches=all_matches
        )

    def group_by_date(
        self,
        signals: AggregatedSignals
    ) -> dict[date, list[dict]]:
        """Group signals by date for timeline view"""
        grouped = defaultdict(list)
        for match in signals.matches:
            d = match['timestamp'].date()
            grouped[d].append(match)
        return dict(grouped)

    def get_statistics(
        self,
        signals: AggregatedSignals
    ) -> SignalStatistics:
        """Compute statistics on signals"""
        if not signals.matches:
            return SignalStatistics(
                total=0,
                by_symbol={},
                by_date={}
            )

        by_symbol = Counter(m['symbol'] for m in signals.matches)
        by_date = Counter(m['timestamp'].date() for m in signals.matches)

        return SignalStatistics(
            total=signals.total_matches,
            by_symbol=dict(by_symbol),
            by_date=dict(by_date)
        )
```

---

## 5. Caching Layer

### 5.1 Indicator Cache

```python
class IndicatorCache:
    """Cache computed indicators to avoid recomputation"""

    def __init__(self, max_size_mb: int = 500):
        self._cache: dict[str, pd.Series] = {}
        self._access_times: dict[str, float] = {}
        self._max_size = max_size_mb * 1024 * 1024  # Convert to bytes
        self._lock = threading.RLock()

    def get(self, key: str) -> Optional[pd.Series]:
        """Get cached indicator"""
        with self._lock:
            if key in self._cache:
                self._access_times[key] = time.time()
                return self._cache[key]
        return None

    def set(self, key: str, value: pd.Series) -> None:
        """Cache indicator value"""
        with self._lock:
            # Evict if necessary
            while self._current_size() + self._series_size(value) > self._max_size:
                self._evict_lru()

            self._cache[key] = value
            self._access_times[key] = time.time()

    def make_key(
        self,
        symbol: str,
        timeframe: str,
        indicator: str,
        period: int = None
    ) -> str:
        """Create cache key"""
        parts = [symbol, timeframe, indicator]
        if period:
            parts.append(str(period))
        return ':'.join(parts)

    def _current_size(self) -> int:
        """Estimate current cache size in bytes"""
        return sum(self._series_size(s) for s in self._cache.values())

    def _series_size(self, series: pd.Series) -> int:
        """Estimate series size in bytes"""
        return series.memory_usage(deep=True)

    def _evict_lru(self) -> None:
        """Evict least recently used entry"""
        if not self._cache:
            return
        lru_key = min(self._access_times, key=self._access_times.get)
        del self._cache[lru_key]
        del self._access_times[lru_key]
```

### 5.2 Result Cache

```python
class ProcessingResultCache:
    """Cache processing results for quick retrieval"""

    def __init__(self, ttl_seconds: int = 300):
        self._cache: dict[str, tuple[ProcessingResult, float]] = {}
        self._ttl = ttl_seconds

    def get(self, job_id: str) -> Optional[ProcessingResult]:
        """Get cached result if not expired"""
        if job_id in self._cache:
            result, timestamp = self._cache[job_id]
            if time.time() - timestamp < self._ttl:
                return result
            else:
                del self._cache[job_id]
        return None

    def set(self, job_id: str, result: ProcessingResult) -> None:
        """Cache result"""
        self._cache[job_id] = (result, time.time())

    def clear_expired(self) -> int:
        """Remove expired entries, return count removed"""
        now = time.time()
        expired = [k for k, (_, ts) in self._cache.items()
                  if now - ts >= self._ttl]
        for k in expired:
            del self._cache[k]
        return len(expired)
```

---

## 6. Streaming Processing

### 6.1 Stream Processor (for real-time)

```python
class StreamProcessor:
    """Process incoming candles in real-time"""

    def __init__(
        self,
        formula_service: FormulaService,
        indicator_pipeline: IndicatorPipeline
    ):
        self._formula_service = formula_service
        self._indicators = indicator_pipeline
        self._buffers: dict[str, deque] = {}  # Symbol -> candle buffer
        self._buffer_size = 500  # Keep last 500 candles

    def on_candle(
        self,
        symbol: str,
        candle: Candle,
        formulas: list[Formula]
    ) -> list[SignalMatch]:
        """Process new candle and check formulas"""
        # Update buffer
        if symbol not in self._buffers:
            self._buffers[symbol] = deque(maxlen=self._buffer_size)
        self._buffers[symbol].append(candle)

        # Need minimum candles for indicators
        if len(self._buffers[symbol]) < 200:
            return []

        # Create DataFrame from buffer
        df = pd.DataFrame([asdict(c) for c in self._buffers[symbol]])
        df.set_index('timestamp', inplace=True)

        # Check formulas on latest candle
        matches = []
        for formula in formulas:
            result = self._formula_service.evaluate(formula, df)

            # Check only the latest value
            if result.iloc[-1]:
                matches.append(SignalMatch(
                    timestamp=candle.timestamp,
                    candle=candle,
                    signal_type=SignalType.SCAN_HIT,
                    indicator_values=self._extract_latest_indicators(df)
                ))

        return matches

    def _extract_latest_indicators(self, df: pd.DataFrame) -> dict[str, float]:
        """Get indicator values at latest candle"""
        indicators = self._indicators.compute(df)
        return {name: float(series.iloc[-1])
               for name, series in indicators.items()
               if not pd.isna(series.iloc[-1])}
```

---

## 7. Use Cases

### 7.1 Batch Scan Use Case

```python
class BatchScanUseCase:
    """Execute formula scan across multiple symbols"""

    def __init__(
        self,
        engine: ProcessingEngine,
        symbol_repo: ISymbolRepository
    ):
        self._engine = engine
        self._symbol_repo = symbol_repo

    async def execute(
        self,
        request: BatchScanRequest,
        progress_callback: Callable[[int, int], None] = None
    ) -> BatchScanResult:
        """Run batch scan"""
        # Resolve symbols
        symbols = await self._resolve_symbols(request)

        # Create job
        job = ProcessingJob(
            id=str(uuid.uuid4()),
            job_type=JobType.SCAN,
            symbols=symbols,
            timeframe=request.timeframe,
            date_range=request.date_range,
            formula=request.formula,
            parameters=request.parameters
        )

        # Execute
        results = await self._engine.process_batch(job, progress_callback)

        # Aggregate
        aggregator = SignalAggregator()
        signals = aggregator.aggregate(results)
        stats = aggregator.get_statistics(signals)

        return BatchScanResult(
            job_id=job.id,
            signals=signals,
            statistics=stats,
            processing_time_ms=sum(r.processing_time_ms for r in results)
        )
```

---

## 8. Tasks Breakdown

| Task ID | Title | Effort | Dependencies |
|---------|-------|--------|--------------|
| E06-F01-T01 | Define ProcessingJob and ProcessingResult | S | E02 |
| E06-F01-T02 | Define ProcessingContext | S | T01 |
| E06-F02-T01 | Implement ProcessingEngine core | L | F01, E05 |
| E06-F02-T02 | Implement parallel processing with ProcessPool | L | F02-T01 |
| E06-F02-T03 | Add progress tracking and cancellation | M | F02-T02 |
| E06-F03-T01 | Implement IndicatorPipeline | M | - |
| E06-F03-T02 | Register all PRD indicators | M | F03-T01 |
| E06-F04-T01 | Implement SignalAggregator | M | F01 |
| E06-F04-T02 | Implement grouping and statistics | M | F04-T01 |
| E06-F05-T01 | Implement IndicatorCache | M | F03 |
| E06-F05-T02 | Implement ProcessingResultCache | M | F02 |
| E06-F06-T01 | Implement StreamProcessor | L | F02, F03 |
| E06-F07-T01 | Create BatchScanUseCase | M | F02, F04 |

---

## 9. Acceptance Criteria

### 9.1 Performance

- [ ] Process 2,500 symbols in < 30 seconds (4 cores)
- [ ] Single symbol processing < 50ms
- [ ] Memory usage < 2GB during batch processing
- [ ] Indicator cache hit rate > 80% for repeated scans

### 9.2 Reliability

- [ ] Graceful handling of symbol with no data
- [ ] Timeout for stuck processing jobs
- [ ] Memory limits enforced on caches

### 9.3 Testing

- [ ] Unit tests for ProcessingEngine with mock data
- [ ] Performance benchmarks with realistic dataset
- [ ] Test coverage > 85%

---

## 10. TDD Approach

### 10.1 Engine Tests

```python
class TestProcessingEngine:
    @pytest.fixture
    def engine(self):
        candle_repo = InMemoryCandleRepository()
        formula_service = FormulaService(InMemoryFormulaRepository())

        # Pre-populate test data
        candles = generate_test_candles(100)
        candle_repo.save_many(1, "1d", candles)

        return ProcessingEngine(candle_repo, formula_service, max_workers=2)

    async def test_processes_multiple_symbols(self, engine):
        job = ProcessingJob(
            id="test-1",
            job_type=JobType.SCAN,
            symbols=["005930", "000660"],
            timeframe="1d",
            date_range=DateRange(date(2024, 1, 1), date(2024, 12, 31)),
            formula=Formula(
                id=1, name="Test",
                expression="close > sma(close, 20)",
                category=FormulaCategory.SCAN
            )
        )

        results = await engine.process_batch(job)

        assert len(results) == 2
        assert all(r.error is None for r in results)

    async def test_handles_missing_data_gracefully(self, engine):
        job = ProcessingJob(
            id="test-2",
            job_type=JobType.SCAN,
            symbols=["UNKNOWN"],
            timeframe="1d",
            date_range=DateRange(date(2024, 1, 1), date(2024, 12, 31)),
            formula=None
        )

        results = await engine.process_batch(job)

        assert len(results) == 1
        assert results[0].error == "No data available"
```

### 10.2 Cache Tests

```python
class TestIndicatorCache:
    def test_caches_and_retrieves(self):
        cache = IndicatorCache(max_size_mb=100)
        key = cache.make_key("005930", "1d", "sma", 20)
        values = pd.Series([1.0, 2.0, 3.0])

        cache.set(key, values)
        result = cache.get(key)

        assert result is not None
        pd.testing.assert_series_equal(result, values)

    def test_evicts_lru_when_full(self):
        cache = IndicatorCache(max_size_mb=1)  # Very small cache

        # Fill cache
        for i in range(1000):
            key = f"key_{i}"
            cache.set(key, pd.Series(range(1000)))

        # Early keys should be evicted
        assert cache.get("key_0") is None
```

---

## 11. References

- E05: DSL Parser & Evaluator (formula evaluation)
- E02: Storage Layer (candle data access)
- [Python ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html)
- [pandas Performance Tips](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)
