# Spec: E11-F02-T02 - Parallel Backtest Execution

---
# ============================================================================
# SPEC METADATA
# ============================================================================

# === IDENTIFICATION ===
id: E11-F02-T02
clickup_task_id: ''
title: Parallel Backtest Execution
type: task

# === HIERARCHY ===
parent: E11-F02
children: []
epic: E11
feature: F02
task: T02
domain: optimization

# === WORKFLOW ===
status: draft
priority: high

# === TRACKING ===
created: '2025-12-30'
updated: '2025-12-30'
due_date: ''
estimated_hours: 5
actual_hours: 0

# === METADATA ===
tags: [parallel, multiprocessing, backtest, executor]
effort: large
risk: medium
wave: 5
parallel: true
---

**Status**: Draft
**Type**: Task
**Parent**: E11-F02 (Grid Search Optimizer Engine)
**Wave**: 5 (Depends on T01)

## Executive Summary

Implement parallel backtest execution using ProcessPoolExecutor. Submit all parameter combinations as parallel jobs, collect results with timeout handling, and call progress callback for each completed backtest. Handle failures gracefully without stopping the optimization.

## User Stories

**As a** trader
**I want to** test 100 parameter combinations quickly
**So that** I can find optimal parameters in minutes, not hours

## Acceptance Scenarios

### Scenario 1: Parallel Execution
**Given** 100 parameter combinations and 8 CPU cores
**When** optimization runs
**Then** up to 8 backtests run simultaneously
**And** all 100 combinations are tested

### Scenario 2: Progress Callback
**Given** optimization running with progress callback
**When** each backtest completes
**Then** callback receives (completed_count, total_count)

### Scenario 3: Failure Handling
**Given** a parameter combination that causes backtest error
**When** the error occurs
**Then** error is logged
**And** optimization continues with remaining combinations
**And** failed combination is not in results

### Scenario 4: Timeout Handling
**Given** a backtest that exceeds 300s timeout
**When** timeout occurs
**Then** timeout is logged
**And** optimization continues

## Requirements

### Functional Requirements
- **FR-001**: MUST execute backtests in parallel using ProcessPoolExecutor
- **FR-002**: MUST call progress_callback(completed, total) for each completion
- **FR-003**: MUST handle individual backtest failures gracefully
- **FR-004**: MUST enforce 300s timeout per backtest
- **FR-005**: MUST collect successful results into list

### Non-Functional Requirements
- **NFR-001**: Performance: 100 combinations in < 5 minutes (PRD 7.1)
- **NFR-002**: Utilize all available CPU cores efficiently

### Technical Constraints
- **TC-001**: Use ProcessPoolExecutor for CPU-bound parallelism
- **TC-002**: Use future.result(timeout=300) for timeout
- **TC-003**: Ensure BacktestEngine can be pickled or use spawn context

## Implementation Details

### File Location
`src/domain/services/optimization/grid_search_optimizer.py`

### Code Structure (Addition to T01)
```python
import logging
import time
from concurrent.futures import ProcessPoolExecutor, TimeoutError as FutureTimeout

class GridSearchOptimizer:
    # ... (from T01)

    async def optimize(
        self,
        config: OptimizationConfig,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> OptimizationResult:
        """Run grid search optimization with parallel execution"""
        start_time = time.perf_counter()

        combinations = self._generate_combinations(config.parameters)
        total = len(combinations)

        # Submit all jobs
        with ProcessPoolExecutor(max_workers=self._max_workers) as executor:
            futures = []
            for params in combinations:
                strategy = self._apply_params(config.strategy, params)
                backtest_config = BacktestConfig(
                    strategy=strategy,
                    symbols=config.symbols,
                    start_date=config.date_range.start,
                    end_date=config.date_range.end
                )
                future = executor.submit(self._run_single_backtest, backtest_config)
                futures.append((params, future))

            # Collect results
            results = []
            completed = 0
            for params, future in futures:
                try:
                    backtest_result = future.result(timeout=300)
                    results.append(ParameterResult(
                        parameters=params,
                        metrics=backtest_result.metrics,
                        rank=0  # Set in ranking step
                    ))
                except FutureTimeout:
                    logging.error(f"Backtest timeout for {params}")
                except Exception as e:
                    logging.error(f"Backtest failed for {params}: {e}")

                completed += 1
                if progress_callback:
                    progress_callback(completed, total)

        # Ranking and walk-forward in T03
        results = self._rank_results(results, config.optimization_target)

        elapsed_ms = (time.perf_counter() - start_time) * 1000

        return OptimizationResult.create(
            config=config,
            results=results,
            execution_time_ms=elapsed_ms
        )

    def _run_single_backtest(self, config: 'BacktestConfig') -> 'BacktestResult':
        """Run a single backtest - callable from worker process"""
        # Note: This runs in a separate process
        return self._backtest_engine.run_sync(config)
```

## Dependencies

### Upstream
- E11-F02-T01: GridSearchOptimizer class with _generate_combinations
- E10: BacktestEngine, BacktestConfig, BacktestResult

### Downstream
- E11-F02-T03: Ranking depends on results collection

## Success Criteria

- [ ] Parallel execution uses ProcessPoolExecutor
- [ ] Progress callback called correctly
- [ ] Timeout handling works (300s)
- [ ] Error handling continues optimization
- [ ] Performance: 100 combinations in < 5 minutes
- [ ] Unit tests with mock executor

---
*Template Version: 2.0.0*
